{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9760651e-b644-430b-8be3-b12ac1791ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Dict, Any\n",
    "from pydantic import BaseModel\n",
    "import json\n",
    "\n",
    "# Re-define models after code state reset\n",
    "class ResumeItem(BaseModel):\n",
    "    title: str\n",
    "    subtitle: Optional[str] = None\n",
    "    start_date: Optional[str] = None\n",
    "    end_date: Optional[str] = None\n",
    "    details: Optional[List[str]] = None\n",
    "    extra: Optional[Dict[str, Any]] = None\n",
    "\n",
    "class ResumeSection(BaseModel):\n",
    "    section_name: str\n",
    "    items: List[ResumeItem]\n",
    "\n",
    "class Resume(BaseModel):\n",
    "    sections: List[ResumeSection]\n",
    "\n",
    "# Sample resume data reflecting output1 style\n",
    "sample_resume = Resume(sections=[\n",
    "    ResumeSection(\n",
    "        section_name=\"Education\",\n",
    "        items=[\n",
    "            ResumeItem(\n",
    "                title=\"PhD candidate in computer science\",\n",
    "                subtitle=\"Old Dominion University | Norfolk, USA\",\n",
    "                start_date=\"Aug 2019\",\n",
    "                end_date=\"present\",\n",
    "                details=[\n",
    "                    \"GPA: 3.84/4.0\",\n",
    "                    \"Research Interests: Self-Supervised Learning and Semi-supervised Learning in Imbalanced datasets (Image, Text and Tabular Domains)\"\n",
    "                ]\n",
    "            ),\n",
    "            ResumeItem(\n",
    "                title=\"Master of Science in Computer Engineering\",\n",
    "                subtitle=\"University of NabiAkram | Tabriz, Iran\"\n",
    "            ),\n",
    "            ResumeItem(\n",
    "                title=\"Bachelor of Science in Computer Engineering\",\n",
    "                subtitle=\"University of Shamsipoor | Tehran, Iran\"\n",
    "            ),\n",
    "        ]\n",
    "    ),\n",
    "    ResumeSection(\n",
    "        section_name=\"Technical Skills\",\n",
    "        items=[\n",
    "            ResumeItem(title=\"Languages & databases\", details=[\"python\", \"Java\", \"C++\", \"ASP Webform\", \"C#\", \"SQL\", \"MySQL\", \"HTML\"]),\n",
    "            ResumeItem(title=\"Libraries\", details=[\"Tensorflow\", \"Keras\", \"PyTorch\", \"OpenCV\", \"Scikit-learn\", \"NLP toolkit\", \"HuggingFace\", \"Pandas\", \"Matplotlib\", \"Seaborn\", \"LangChain\", \"Dask\", \"BeautifulSoup\", \"Flask\"]),\n",
    "            ResumeItem(title=\"Development tools\", details=[\"Anaconda\", \"Jupyter Notebook\", \"Google Colab\", \"Visual Studio\", \"Git\", \"Docker\", \"AWS\"]),\n",
    "            ResumeItem(title=\"Operating Systems\", details=[\"Windows\", \"Linux\", \"Mac OS X\"]),\n",
    "            ResumeItem(title=\"Certifications\", details=[\"LanGraph: Link\", \"LLM Engineering: Link\", \"AWS SageMaker: Link\"]),\n",
    "            ResumeItem(title=\"Awards and Honors\", details=[\"Received \\\"Best Teaching Assistant\\\" award, Spring 2025\"])\n",
    "        ]\n",
    "    )\n",
    "])\n",
    "\n",
    "# Serialize to JSON and save\n",
    "resume_json = sample_resume.model_dump_json(indent=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e47a10a2-4a62-49da-9d4e-8ce9217d2baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resume_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e30aeb3-0dd3-4673-bae5-b0e1e365e6aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'generated_resume.docx'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from docx import Document\n",
    "from docx.shared import Pt\n",
    "from docx.enum.text import WD_PARAGRAPH_ALIGNMENT\n",
    "from flask import Response\n",
    "from bs4 import BeautifulSoup\n",
    "import io\n",
    "import json\n",
    "\n",
    "# Load resume JSON\n",
    "# with open(\"/mnt/data/sample_resume.json\", \"r\") as f:\n",
    "#     resume_data = json.load(f)\n",
    "# resume_json = '{\"sections\": [...]}'\n",
    "resume_dict = json.loads(resume_json)\n",
    "resume = Resume.model_validate(resume_dict)\n",
    "# Re-parse with Pydantic\n",
    "# resume = Resume.model_validate(resume_data)\n",
    "\n",
    "# Generate DOCX\n",
    "def generate_docx_from_resume(resume: Resume) -> bytes:\n",
    "    doc = Document()\n",
    "    style = doc.styles['Normal']\n",
    "    font = style.font\n",
    "    font.name = 'Arial'\n",
    "    font.size = Pt(11)\n",
    "\n",
    "    for section in resume.sections:\n",
    "        doc.add_heading(section.section_name, level=1)\n",
    "\n",
    "        for item in section.items:\n",
    "            # Subtitle as job title or university\n",
    "            if item.subtitle:\n",
    "                p = doc.add_paragraph()\n",
    "                run = p.add_run(item.subtitle)\n",
    "                run.bold = True\n",
    "                p.alignment = WD_PARAGRAPH_ALIGNMENT.LEFT\n",
    "\n",
    "            # Title if distinct from subtitle\n",
    "            if item.title and item.title != item.subtitle:\n",
    "                doc.add_paragraph(item.title)\n",
    "\n",
    "            # Date range\n",
    "            if item.start_date or item.end_date:\n",
    "                doc.add_paragraph(f\"{item.start_date or ''} – {item.end_date or ''}\", style='Intense Quote')\n",
    "\n",
    "            # Details: inline for Technical Skills, bullets for others\n",
    "            if item.details:\n",
    "                if \"skill\" in section.section_name.lower():# == \"Technical Skills\":\n",
    "                    doc.add_paragraph(\", \".join(item.details))\n",
    "                else:\n",
    "                    for detail in item.details:\n",
    "                        doc.add_paragraph(detail, style='List Bullet')\n",
    "\n",
    "    buffer = io.BytesIO()\n",
    "    doc.save(buffer)\n",
    "    buffer.seek(0)\n",
    "    return buffer.read()\n",
    "\n",
    "# Save DOCX\n",
    "docx_bytes = generate_docx_from_resume(resume)\n",
    "docx_path = \"generated_resume.docx\"\n",
    "with open(docx_path, \"wb\") as f:\n",
    "    f.write(docx_bytes)\n",
    "\n",
    "docx_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23805ab7-da1c-47fc-8585-d7d9ee233bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'generated_resume2.pdf'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from weasyprint import HTML\n",
    "\n",
    "# Convert resume object into simple HTML structure\n",
    "def resume_to_html(resume: Resume) -> str:\n",
    "    html_parts = [\n",
    "        '<html><head><meta charset=\"utf-8\"><style>',\n",
    "        'body { font-family: Arial, sans-serif; line-height: 1.6; font-size: 12pt; }',\n",
    "        'h1 { font-size: 16pt; margin-top: 30px; color: #2c3e50; border-bottom: 1px solid #ccc; }',\n",
    "        'h2 { font-size: 13pt; margin-top: 20px; color: #2c3e50; }',\n",
    "        'ul { margin: 0; padding-left: 20px; }',\n",
    "        'li { margin-bottom: 4px; }',\n",
    "        'p { margin: 5px 0; }',\n",
    "        '</style></head><body>'\n",
    "    ]\n",
    "\n",
    "    for section in resume.sections:\n",
    "        html_parts.append(f\"<h1>{section.section_name}</h1>\")\n",
    "        for item in section.items:\n",
    "            if item.subtitle:\n",
    "                html_parts.append(f\"<h2>{item.subtitle}</h2>\")\n",
    "\n",
    "            if item.title and item.title != item.subtitle:\n",
    "                html_parts.append(f\"<p><strong>{item.title}</strong></p>\")\n",
    "\n",
    "            if item.start_date or item.end_date:\n",
    "                html_parts.append(f\"<p><em>{item.start_date or ''} – {item.end_date or ''}</em></p>\")\n",
    "\n",
    "            if item.details:\n",
    "                if \"skill\" in section.section_name.lower():# == \"Technical Skills\":\n",
    "                    # Inline format for technical skills\n",
    "                    html_parts.append(f\"<p>{', '.join(item.details)}</p>\")\n",
    "                else:\n",
    "                    html_parts.append(\"<ul>\")\n",
    "                    for detail in item.details:\n",
    "                        html_parts.append(f\"<li>{detail}</li>\")\n",
    "                    html_parts.append(\"</ul>\")\n",
    "\n",
    "    html_parts.append('</body></html>')\n",
    "    return ''.join(html_parts)\n",
    "# Generate and save PDF\n",
    "html_content = resume_to_html(resume)\n",
    "pdf_path = \"generated_resume2.pdf\"\n",
    "HTML(string=html_content).write_pdf(pdf_path)\n",
    "\n",
    "pdf_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "13c479fb-206e-49c3-8ebc-8eaac726aeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "res=Resume(sections=[ResumeSection(section_name='Education', items=[ResumeItem(title='Ph.D. in Computer Science', subtitle='Old Dominion University, VA, USA', start_date='August 2019', end_date='Present', details=['GPA: 3.84/4.0', 'Research Interest: Natural Language Processing (NLP), Computer Vision (CV), Machine Learning (ML), Deep Learning (DL), Digital Libraries, and Scholarly Big Data'], extra=None), ResumeItem(title='Bachelor of Science in Computer Engineering', subtitle='Elizabethtown College, PA, USA', start_date='August 2014', end_date='May 2018', details=['GPA: 3.36/4.0', 'Minor: Information Systems'], extra=None)]), ResumeSection(section_name='Technical Skills', items=[ResumeItem(title='Languages & Database', subtitle=None, start_date=None, end_date=None, details=['Python', 'PHP', 'C', 'HTML', 'CSS', 'SQL', 'MySQL', 'AWS S3'], extra=None), ResumeItem(title='Technologies & APIs', subtitle=None, start_date=None, end_date=None, details=['Keras', 'Tensorflow', 'PyTorch', 'OpenCV', 'scikit-learn', 'NLP toolkit'], extra=None), ResumeItem(title='Development Tools', subtitle=None, start_date=None, end_date=None, details=['Anaconda', 'Jupyter Notebook', 'Google Colab', 'Visual Studio', 'SVN', 'Git', 'Docker', 'AWS'], extra=None), ResumeItem(title='Operating Systems', subtitle=None, start_date=None, end_date=None, details=['Linux', 'Mac OS X', 'Windows Server'], extra=None)]), ResumeSection(section_name='Experience', items=[ResumeItem(title='Graduate Research Assistant', subtitle='Old Dominion University', start_date='August 2019', end_date='Present', details=['Developing AI-based applications using ML and DL by leveraging NLP and CV.', 'Conducting research on Digital Libraries, Computational Reproducibility and Replicability.', 'Mentoring students, writing research papers, and presenting research work at top conferences and journals.'], extra={}), ResumeItem(title='Research Intern', subtitle='Los Alamos National Laboratory', start_date='June 2020', end_date='August 2020', details=['Conducted research and implemented a framework for offline handwritten mathematical equation recognition.', 'Preprocessed images, built ground truth, applied OpenCV for segmentation, blurring, and binary thresholding.', 'Employed deep neural networks such as LeNET5-CNN as a model backbone and achieved 89% model accuracy.'], extra=None), ResumeItem(title='Machine Learning Intern', subtitle='Bihrle Applied Research Inc', start_date='June 2021', end_date='August 2021', details=['Developed and enhanced algorithms for Train Detection used by Rail-Inspector – a cloud-based software that processes aerial imagery of railroad tracks using machine learning and deep learning.', 'Built ground truth by labeling images of trains, employed deep learning model such as FCN for segmentation.', 'Trained the model, solved overfitting problems, optimized the result, and achieved 96% accuracy.'], extra=None)]), ResumeSection(section_name='Projects', items=[ResumeItem(title='AutoMeta', subtitle=None, start_date='August 2019', end_date='January 2021', details=['A metadata extractor application to extract metadata fields from scanned book-length documents such as electronic theses and dissertations (ETDs) by leveraging NLP techniques.', 'It uses ML-based methods such as Conditional Random Field (CRF), which incorporates text and visual features.', 'The model was trained and evaluated using AutoMeta-ETD500, and achieved F1 score of 83% – 96%.'], extra={}), ResumeItem(title='ETDPC', subtitle=None, start_date='March 2021', end_date='August 2023', details=['A two-stream novel multi-modal classification model with cross-attention that uses vision encoder (ResNet50v2) and text encoder (BERT with Talking-Heads Attention) to classify ETD pages into 13 categories.', 'The model was trained and evaluated using ETDPC-ETD500, and achieved F1 score of 84% – 96%.'], extra={}), ResumeItem(title='MetaEnhance', subtitle=None, start_date='May 2022', end_date='December 2022', details=['An application to improve the metadata quality of ETDs by filling out the missing values, correcting the incorrect values and misspellings, and canonicalizing the surface values by leveraging the SOTA ML and DL models.', 'The framework was evaluated against MetaEnhance-ETDQual500 and achieved nearly perfect F1-scores in detecting errors and F1-scores ranging from 85% – 100% for correcting five of seven key metadata fields.'], extra={})]), ResumeSection(section_name='Publications', items=[ResumeItem(title='MetaEnhance: Metadata Quality Improvement for ETDs of University Libraries.', subtitle='JCDL 2023', start_date=None, end_date=None, details=['Link (Best Paper Award)'], extra=None), ResumeItem(title='A Study on Reproducibility and Replicability of Table Structure Recognition Methods.', subtitle='ICDAR 2023', start_date=None, end_date=None, details=['Link'], extra=None), ResumeItem(title='A Study of Computational Reproducibility using URLs Linking to Open Access Datasets and Software.', subtitle='WWW 2022', start_date=None, end_date=None, details=['Link'], extra=None), ResumeItem(title='Segmenting Technical Drawing Figures in US Patents.', subtitle='SDU@AAAI 2022', start_date=None, end_date=None, details=['Link'], extra=None), ResumeItem(title='Automatic Metadata Extraction Incorporating Visual Features from Scanned ETDs.', subtitle='JCDL 2021', start_date=None, end_date=None, details=['Link'], extra=None), ResumeItem(title='A Heuristic Baseline Method for Metadata Extraction from Scanned ETDs.', subtitle='JCDL 2020', start_date=None, end_date=None, details=['Link (Best Poster Award)'], extra=None)])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9ca2cd22-a3d6-47c4-aeea-0146cde96682",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_reactive_resume(resume: Resume) -> Dict[str, Any]:\n",
    "    sections_title = []\n",
    "    for sec in resume.sections:\n",
    "        section_name = sec.section_name.strip()\n",
    "        sections_title.append(section_name)\n",
    "        # for item in sec.items:\n",
    "        #     data = {\n",
    "        #             \"name\": item.title,\n",
    "        #             \"position\": item.subtitle or \"\",\n",
    "        #             \"startDate\": item.start_date or \"\",\n",
    "        #             \"endDate\": item.end_date or \"\",\n",
    "        #             \"highlights\": item.details or [],\n",
    "        #             \"summary\": \"\",\n",
    "        #             \"url\": \"\",\n",
    "        #             \"keywords\": []\n",
    "        #         }\n",
    "    reactive = { \"basics\": {\n",
    "                \"name\": \"\",\n",
    "                \"email\": \"\",\n",
    "                \"phone\": \"\",\n",
    "                \"location\": {\"city\": \"\", \"region\": \"\", \"country\": \"\"},\n",
    "                \"url\": \"\",\n",
    "                \"profiles\": []\n",
    "            },}\n",
    "    for S in sections_title:\n",
    "        reactive[S]=[]\n",
    "    \n",
    "    for section in resume.sections:\n",
    "            section_name = section.section_name.strip()\n",
    "    \n",
    "            for item in section.items:\n",
    "                data = {\n",
    "                    \"name\": item.title,\n",
    "                    \"position\": item.subtitle or \"\",\n",
    "                    \"startDate\": item.start_date or \"\",\n",
    "                    \"endDate\": item.end_date or \"\",\n",
    "                    \"highlights\": item.details or [],\n",
    "                    \"summary\": \"\",\n",
    "                    \"url\": \"\",\n",
    "                    \"keywords\": []\n",
    "                }\n",
    "                reactive[section_name].append(data)\n",
    "    return reactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f57b4379-d50c-4e7e-bb3e-fc6eac76a580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'basics': {'name': '',\n",
       "  'email': '',\n",
       "  'phone': '',\n",
       "  'location': {'city': '', 'region': '', 'country': ''},\n",
       "  'url': '',\n",
       "  'profiles': []},\n",
       " 'Education': [{'name': 'Ph.D. in Computer Science',\n",
       "   'position': 'Old Dominion University, VA, USA',\n",
       "   'startDate': 'August 2019',\n",
       "   'endDate': 'Present',\n",
       "   'highlights': ['GPA: 3.84/4.0',\n",
       "    'Research Interest: Natural Language Processing (NLP), Computer Vision (CV), Machine Learning (ML), Deep Learning (DL), Digital Libraries, and Scholarly Big Data'],\n",
       "   'summary': '',\n",
       "   'url': '',\n",
       "   'keywords': []},\n",
       "  {'name': 'Bachelor of Science in Computer Engineering',\n",
       "   'position': 'Elizabethtown College, PA, USA',\n",
       "   'startDate': 'August 2014',\n",
       "   'endDate': 'May 2018',\n",
       "   'highlights': ['GPA: 3.36/4.0', 'Minor: Information Systems'],\n",
       "   'summary': '',\n",
       "   'url': '',\n",
       "   'keywords': []}],\n",
       " 'Technical Skills': [{'name': 'Languages & Database',\n",
       "   'position': '',\n",
       "   'startDate': '',\n",
       "   'endDate': '',\n",
       "   'highlights': ['Python',\n",
       "    'PHP',\n",
       "    'C',\n",
       "    'HTML',\n",
       "    'CSS',\n",
       "    'SQL',\n",
       "    'MySQL',\n",
       "    'AWS S3'],\n",
       "   'summary': '',\n",
       "   'url': '',\n",
       "   'keywords': []},\n",
       "  {'name': 'Technologies & APIs',\n",
       "   'position': '',\n",
       "   'startDate': '',\n",
       "   'endDate': '',\n",
       "   'highlights': ['Keras',\n",
       "    'Tensorflow',\n",
       "    'PyTorch',\n",
       "    'OpenCV',\n",
       "    'scikit-learn',\n",
       "    'NLP toolkit'],\n",
       "   'summary': '',\n",
       "   'url': '',\n",
       "   'keywords': []},\n",
       "  {'name': 'Development Tools',\n",
       "   'position': '',\n",
       "   'startDate': '',\n",
       "   'endDate': '',\n",
       "   'highlights': ['Anaconda',\n",
       "    'Jupyter Notebook',\n",
       "    'Google Colab',\n",
       "    'Visual Studio',\n",
       "    'SVN',\n",
       "    'Git',\n",
       "    'Docker',\n",
       "    'AWS'],\n",
       "   'summary': '',\n",
       "   'url': '',\n",
       "   'keywords': []},\n",
       "  {'name': 'Operating Systems',\n",
       "   'position': '',\n",
       "   'startDate': '',\n",
       "   'endDate': '',\n",
       "   'highlights': ['Linux', 'Mac OS X', 'Windows Server'],\n",
       "   'summary': '',\n",
       "   'url': '',\n",
       "   'keywords': []}],\n",
       " 'Experience': [{'name': 'Graduate Research Assistant',\n",
       "   'position': 'Old Dominion University',\n",
       "   'startDate': 'August 2019',\n",
       "   'endDate': 'Present',\n",
       "   'highlights': ['Developing AI-based applications using ML and DL by leveraging NLP and CV.',\n",
       "    'Conducting research on Digital Libraries, Computational Reproducibility and Replicability.',\n",
       "    'Mentoring students, writing research papers, and presenting research work at top conferences and journals.'],\n",
       "   'summary': '',\n",
       "   'url': '',\n",
       "   'keywords': []},\n",
       "  {'name': 'Research Intern',\n",
       "   'position': 'Los Alamos National Laboratory',\n",
       "   'startDate': 'June 2020',\n",
       "   'endDate': 'August 2020',\n",
       "   'highlights': ['Conducted research and implemented a framework for offline handwritten mathematical equation recognition.',\n",
       "    'Preprocessed images, built ground truth, applied OpenCV for segmentation, blurring, and binary thresholding.',\n",
       "    'Employed deep neural networks such as LeNET5-CNN as a model backbone and achieved 89% model accuracy.'],\n",
       "   'summary': '',\n",
       "   'url': '',\n",
       "   'keywords': []},\n",
       "  {'name': 'Machine Learning Intern',\n",
       "   'position': 'Bihrle Applied Research Inc',\n",
       "   'startDate': 'June 2021',\n",
       "   'endDate': 'August 2021',\n",
       "   'highlights': ['Developed and enhanced algorithms for Train Detection used by Rail-Inspector – a cloud-based software that processes aerial imagery of railroad tracks using machine learning and deep learning.',\n",
       "    'Built ground truth by labeling images of trains, employed deep learning model such as FCN for segmentation.',\n",
       "    'Trained the model, solved overfitting problems, optimized the result, and achieved 96% accuracy.'],\n",
       "   'summary': '',\n",
       "   'url': '',\n",
       "   'keywords': []}],\n",
       " 'Projects': [{'name': 'AutoMeta',\n",
       "   'position': '',\n",
       "   'startDate': 'August 2019',\n",
       "   'endDate': 'January 2021',\n",
       "   'highlights': ['A metadata extractor application to extract metadata fields from scanned book-length documents such as electronic theses and dissertations (ETDs) by leveraging NLP techniques.',\n",
       "    'It uses ML-based methods such as Conditional Random Field (CRF), which incorporates text and visual features.',\n",
       "    'The model was trained and evaluated using AutoMeta-ETD500, and achieved F1 score of 83% – 96%.'],\n",
       "   'summary': '',\n",
       "   'url': '',\n",
       "   'keywords': []},\n",
       "  {'name': 'ETDPC',\n",
       "   'position': '',\n",
       "   'startDate': 'March 2021',\n",
       "   'endDate': 'August 2023',\n",
       "   'highlights': ['A two-stream novel multi-modal classification model with cross-attention that uses vision encoder (ResNet50v2) and text encoder (BERT with Talking-Heads Attention) to classify ETD pages into 13 categories.',\n",
       "    'The model was trained and evaluated using ETDPC-ETD500, and achieved F1 score of 84% – 96%.'],\n",
       "   'summary': '',\n",
       "   'url': '',\n",
       "   'keywords': []},\n",
       "  {'name': 'MetaEnhance',\n",
       "   'position': '',\n",
       "   'startDate': 'May 2022',\n",
       "   'endDate': 'December 2022',\n",
       "   'highlights': ['An application to improve the metadata quality of ETDs by filling out the missing values, correcting the incorrect values and misspellings, and canonicalizing the surface values by leveraging the SOTA ML and DL models.',\n",
       "    'The framework was evaluated against MetaEnhance-ETDQual500 and achieved nearly perfect F1-scores in detecting errors and F1-scores ranging from 85% – 100% for correcting five of seven key metadata fields.'],\n",
       "   'summary': '',\n",
       "   'url': '',\n",
       "   'keywords': []}],\n",
       " 'Publications': [{'name': 'MetaEnhance: Metadata Quality Improvement for ETDs of University Libraries.',\n",
       "   'position': 'JCDL 2023',\n",
       "   'startDate': '',\n",
       "   'endDate': '',\n",
       "   'highlights': ['Link (Best Paper Award)'],\n",
       "   'summary': '',\n",
       "   'url': '',\n",
       "   'keywords': []},\n",
       "  {'name': 'A Study on Reproducibility and Replicability of Table Structure Recognition Methods.',\n",
       "   'position': 'ICDAR 2023',\n",
       "   'startDate': '',\n",
       "   'endDate': '',\n",
       "   'highlights': ['Link'],\n",
       "   'summary': '',\n",
       "   'url': '',\n",
       "   'keywords': []},\n",
       "  {'name': 'A Study of Computational Reproducibility using URLs Linking to Open Access Datasets and Software.',\n",
       "   'position': 'WWW 2022',\n",
       "   'startDate': '',\n",
       "   'endDate': '',\n",
       "   'highlights': ['Link'],\n",
       "   'summary': '',\n",
       "   'url': '',\n",
       "   'keywords': []},\n",
       "  {'name': 'Segmenting Technical Drawing Figures in US Patents.',\n",
       "   'position': 'SDU@AAAI 2022',\n",
       "   'startDate': '',\n",
       "   'endDate': '',\n",
       "   'highlights': ['Link'],\n",
       "   'summary': '',\n",
       "   'url': '',\n",
       "   'keywords': []},\n",
       "  {'name': 'Automatic Metadata Extraction Incorporating Visual Features from Scanned ETDs.',\n",
       "   'position': 'JCDL 2021',\n",
       "   'startDate': '',\n",
       "   'endDate': '',\n",
       "   'highlights': ['Link'],\n",
       "   'summary': '',\n",
       "   'url': '',\n",
       "   'keywords': []},\n",
       "  {'name': 'A Heuristic Baseline Method for Metadata Extraction from Scanned ETDs.',\n",
       "   'position': 'JCDL 2020',\n",
       "   'startDate': '',\n",
       "   'endDate': '',\n",
       "   'highlights': ['Link (Best Poster Award)'],\n",
       "   'summary': '',\n",
       "   'url': '',\n",
       "   'keywords': []}]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_to_reactive_resume(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
