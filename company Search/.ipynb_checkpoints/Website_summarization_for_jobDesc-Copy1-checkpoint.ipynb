{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a16733-a437-442e-832c-5d29e73d723d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c7bbb724-ab57-434d-b472-3f82979277f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import faiss\n",
    "import numpy as np\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ccce99-a9c4-4bc5-9251-61dfd7ba53f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b49126d-9bea-4b75-b6ba-93dace6c8c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Website:\n",
    "    headers = {\n",
    " \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "}\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        response = requests.get(url, headers=self.headers)\n",
    "        self.body = response.content\n",
    "        soup = BeautifulSoup(self.body, 'html.parser')\n",
    "        self.title = soup.title.string if soup.title else \"No title found\"\n",
    "        if soup.body:\n",
    "            for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "                irrelevant.decompose()\n",
    "            self.text = soup.body.get_text(separator=\"\\n\", strip=True)\n",
    "        else:\n",
    "            self.text = \"\"\n",
    "        links = [link.get('href') for link in soup.find_all('a')]\n",
    "        self.links = [link for link in links if link]\n",
    "\n",
    "    def get_contents(self):\n",
    "        return f\"Webpage Title:\\n{self.title}\\nWebpage Contents:\\n{self.text}\\n\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9684660c-8a31-4fc2-b8b3-c086f1f4d738",
   "metadata": {},
   "source": [
    "url='https://www.zoetis.com/'\n",
    "job_description=\"\"\"    Kalamazoo - Downtown Portage Street\n",
    "\n",
    "time type\n",
    "    Full time\n",
    "\n",
    "posted on\n",
    "    Posted 30+ Days Ago\n",
    "\n",
    "job requisition id\n",
    "    JR00016811\n",
    "\n",
    "States considered: All\n",
    "\n",
    "Role Description:\n",
    "\n",
    "precision animal HEALTH GENERATIVE AI Internship\n",
    "\n",
    "Location: Remote but based in Kalamazoo, MI\n",
    "\n",
    "Internship Summary:\n",
    "\n",
    "The ideal candidate will apply machine learning, deep learning, and generative AI approaches to support the development of advanced technologies for use in farm settings. This role will emphasize integrating natural language processing and chatbot functionality to improve the accessibility and usability of data insights for end users. The internship will also involve building end-to-end solutions that link front-end interfaces with back-end data systems. \n",
    "\n",
    "Internship Job Duties:\n",
    "\n",
    "    Gain familiarity with Zoetis Precision Animal Health data collection and recording systems, with a focus on integrating chatbot and NLP technologies.\n",
    "    Contribute to research and development efforts in machine learning, deep learning, and generative AI for product enhancement.\n",
    "    Develop and deploy NLP models to support natural language understanding and generation, enabling improved interactions through chatbots and other interfaces.\n",
    "    Collaborate on building front-end and back-end connectivity to ensure smooth data flow and user interface integration.\n",
    "    Analyze and manage disparate data sources, and clearly communicate technical results and methods.\n",
    "    Interact cross-functionally with internal and external teams to align technical solutions with user needs.\n",
    "\n",
    "Internship Qualifications:\n",
    "\n",
    "Education:\n",
    "\n",
    "    Currently enrolled in a graduate degree program in a quantitative or technical discipline (e.g., computer science, data science, engineering, statistics, statistical genetics, industrial engineering, computational biology, applied mathematics, applied physics, or similar).\n",
    "\n",
    "Technical Skills:\n",
    "\n",
    "    Experience developing, training, and deploying machine learning and deep learning models using public libraries (e.g., TensorFlow, Keras, Sklearn, PyTorch) in Python.\n",
    "    Familiarity with natural language processing (NLP) techniques and models, particularly for applications in chatbot and language generation technologies (e.g., Transformer-based models like BERT, GPT).\n",
    "    Knowledge of generative AI approaches and experience in implementing and fine-tuning generative models (GANs, VAEs, etc.) is a plus.\n",
    "    Proficiency in using APIs for model deployment, including working with tools like the OpenAI GPT API or similar frameworks for integrating NLP capabilities into applications.\n",
    "    Proficiency in front-end and back-end integration, with a working understanding of connecting models with user interfaces and databases to enable end-to-end solutions.\n",
    "\n",
    "General Skills:\n",
    "\n",
    "A background in animal agriculture is desired but not essential.\n",
    "\n",
    "Ability to work independently and collaboratively as part of a team.\n",
    "\n",
    "    Strong communication skills, including the ability to present complex technical methods and results clearly to non-technical audiences.\n",
    "    Demonstrated problem-solving, analytical, and organizational skills.\n",
    "\n",
    "Additional Requirements:\n",
    "\n",
    "    At least 18 years of age and authorized to work in the U.S.\n",
    "    Must be enrolled in a degree program during the Spring term preceding the internship.\n",
    "\n",
    "Successfully pass a background check and drug screen.\n",
    "\n",
    "The following hourly pay rates reflect the anticipated base pay for this position:\n",
    "\n",
    " \n",
    "\n",
    "If the selected candidate is a student pursuing an Associate-level degree: $16.00 per hour\n",
    "\n",
    "If the selected candidate is a student pursuing an Undergraduate-level degree: $20.00 per hour \n",
    "\n",
    "If the selected candidate is a student pursing a Graduate-level degree: $32.50 per hour \n",
    "\n",
    "If the selected candidate is a student pursuing a Doctorate-level degree: $36.00 per hour\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "The following hourly pay rates reflect the anticipated base pay for this position if a selected candidate were to be located in California, Connecticut, District of Columbia, Illinois (Chicago area), Massachusetts, New Jersey, New York, Washington (Seattle area):\n",
    "\n",
    " \n",
    "\n",
    "Student pursuing an Associate-level degree: $17.00 per hour\n",
    "\n",
    "Student pursuing an Undergraduate-level degree: $22.00 per hour \n",
    "\n",
    "Student pursing a Graduate-level degree: $36.40 per hour \n",
    "\n",
    "Student pursuing a Doctorate-level degree: $40.30 per hour\n",
    "\n",
    "Full time\n",
    "\n",
    "Intern (Trainee)\n",
    "\n",
    "Colleague\n",
    "\n",
    "Any unsolicited resumes sent to Zoetis from a third party, such as an Agency recruiter, including unsolicited resumes sent to a Zoetis mailing address, fax machine or email address, directly to Zoetis employees, or to Zoetis resume database will be considered Zoetis property. Zoetis will NOT pay a fee for any placement resulting from the receipt of an unsolicited resume.\n",
    "\n",
    "Zoetis will consider any candidate for whom an Agency has submitted an unsolicited resume to have been referred by the Agency free of any charges or fees. This includes any Agency that is an approved/engaged vendor but does not have the appropriate approvals to be engaged on a search.\n",
    "\n",
    "Zoetis is committed to equal opportunity in the terms and conditions of employment for all employees and job applicants without regard to race, color, religion, sex, sexual orientation, age, gender identity or gender expression, national origin, disability or veteran status or any other protected classification. Disabled individuals are given an equal opportunity to use our online application system. We offer reasonable accommodations as an alternative if requested by an individual with a disability. Please contact Zoetis Colleague Services at zoetiscolleagueservices@zoetis.com to request an accommodation. Zoetis also complies with all applicable national, state and local laws governing nondiscrimination in employment as well as employment eligibility verification requirements of the Immigration and Nationality Act. All applicants must possess or obtain authorization to work in the US for Zoetis. Zoetis retains sole and exclusive discretion to pursue sponsorship for the acquisition or maintenance of nonimmigrant status and employment eligibility, considering factors such as availability of qualified US workers. Individuals requiring sponsorship must disclose this fact. Please note that Zoetis seeks information related to job applications from candidates for jobs in the U.S. solely via the following:  (1) our company website at www.Zoetis.com/careers site, or (2) via email to/from addresses using only the Zoetis domain of “@zoetis.com”. In addition, Zoetis does not use Google Hangout for any recruitment related activities.  Any solicitation or request for information related to job applications with Zoetis via any other means and/or utilizing email addresses with any other domain should be disregarded.  In addition, Zoetis will never ask candidates to make any type of personal financial investment related to gaining employment with Zoetis.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08141687-428e-40b7-8a55-16c25a6ce1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://neuropathbhc.org/'\n",
    "job_description = \"\"\"About the Job\n",
    "We are seeking a motivated AI Intern to join our team and contribute to exciting Projects in machine learning and artificial intelligence. The ideal candidate will have a strong foundation in Web app development and machine learning concepts. This (unpaid) internship provides a unique opportunity to work alongside experienced professionals and gain hands-on experience in developing AI solutions.\n",
    "Job Responsibilities\n",
    "Assist in the design and implementation of machine learning models.\n",
    "Analyze large datasets to extract insights and improve model performance.\n",
    "Collaborate with team members on various AI projects, contributing to data preprocessing, model training, and evaluation.\n",
    "Participate in brainstorming sessions to identify new areas for AI applications.\n",
    "Document processes and results for internal use and reporting.\n",
    "Requirements\n",
    "Current college student, or recent graduate.\n",
    "Applicants must be pursuing or have a completed Bachelor’s degree in Information Technology, Healthcare Technology, or a related field. Those with equivalent work experience will be considered.\n",
    "Graduate’s or Currently pursuing a degree in Computer Science, Information technology, Machine Learning, or a related field.\n",
    "Familiarity with machine learning framework and web applications.\n",
    "Proficiency in programming languages such as, Web application.\n",
    "Basic understanding of AI and machine learning techniques.\n",
    "Strong problem-solving skills and a willingness to learn.\n",
    "Excellent communication and teamwork abilities.\n",
    "Preferred Qualifications\n",
    "AI / Machine knowledge is required.\n",
    "A good hands on knowledge of web applications is good.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6061e5de-266f-44b3-a943-cc5c95e1d450",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SummarizationAgent:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.link_system_prompt = \"You are provided with a list of links found on a webpage. \\\n",
    "        You are able to decide which of the links would be most relevant to include in a brochure about the company, \\\n",
    "        such as links to an About page, or a Company page, or Careers/Jobs pages.\\n\"\n",
    "        \n",
    "        self.link_system_prompt += \"You should respond in JSON as in this example:\"\n",
    "        self.link_system_prompt += \"\"\"\n",
    "        {\n",
    "            \"links\": [\n",
    "                {\"type\": \"about page\", \"url\": \"https://full.url/goes/here/about\"},\n",
    "                {\"type\": \"careers page\": \"url\": \"https://another.full.url/careers\"}\n",
    "            ]\n",
    "        }\n",
    "        \"\"\"\n",
    "\n",
    "        self.link_system_prompt = \"You are provided with a list of links found on a webpage. \\\n",
    "        Your task is to determine which links are most relevant for finding job-related information, \\\n",
    "        including job postings, team structures, related projects, departments, company research, and company news. \\\n",
    "        Prioritize pages that provide context about the job role and its industry impact.\\n\"\n",
    "        self.link_system_prompt += \"You should respond in JSON as in this example:\"\n",
    "        self.link_system_prompt += \"\"\"\n",
    "                {\n",
    "                    \"links\": [\n",
    "                        {\"type\": \"job posting\", \"url\": \"https://full.url/goes/here/job-title\"},\n",
    "                        {\"type\": \"careers page\", \"url\": \"https://full.url/goes/here/careers\"},\n",
    "                        {\"type\": \"team/department page\", \"url\": \"https://full.url/goes/here/team\"},\n",
    "                        {\"type\": \"project/research page\", \"url\": \"https://full.url/goes/here/project\"},\n",
    "                        {\"type\": \"company news\", \"url\": \"https://full.url/goes/here/news\"}\n",
    "                    ]\n",
    "                }\n",
    "        \"\"\"\n",
    "\n",
    "        self.system_prompt = \"You are an assistant that searches for and extracts all available information related to a job posting from a company website. \\\n",
    "                Your goal is to gather as much detail as possible to help the user understand the job role, its responsibilities, \\\n",
    "                the projects it is related to, the team or department it belongs to, and any background about the company’s work in this area. Find anything that can be related to keywords in job description\\\n",
    "                Analyze relevant webpages, including careers, company projects, teams, and news, to provide a comprehensive understanding \\\n",
    "                of the job in its full context. Preserve all relevant text and descriptions while ensuring clarity and completeness. \\\n",
    "                Respond in Markdown format.\"\n",
    "\n",
    "        load_dotenv(override=True)\n",
    "        api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "        self.MODEL = 'gpt-4o-mini'\n",
    "        self.openai = OpenAI()\n",
    "        self.result=''\n",
    "        self.visited=None\n",
    "        \n",
    "\n",
    "    \n",
    "    def generate_search_term(self, job_description):\n",
    "        \"\"\"Use OpenAI to generate an optimized search term based on the job description.\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        Given the following job description, generate the best search term to find relevant information about this role, \n",
    "        including company projects, industry focus, and related news. The search term should be concise and effective for Google search.\n",
    "    \n",
    "        Job Description:\n",
    "        {job_description}\n",
    "    \n",
    "        Provide only the search term without explanation.\n",
    "        \"\"\"\n",
    "    \n",
    "        try:\n",
    "            response=self.openai.chat.completions.create(\n",
    "                model=self.MODEL,\n",
    "                messages=[{\"role\": \"system\", \"content\": \"You are an expert at creating effective Google search queries.\"},\n",
    "                              {\"role\": \"user\", \"content\": prompt}],\n",
    "            )\n",
    "            # response = openai.chat.completions.create(\n",
    "            #     model=MODEL,\n",
    "            #     messages=[{\"role\": \"system\", \"content\": \"You are an expert at creating effective Google search queries.\"},\n",
    "            #               {\"role\": \"user\", \"content\": prompt}],\n",
    "            #     # max_tokens=20\n",
    "            # )\n",
    "            return response.choices[0].message.content \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating search term: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def google_search(self, search_query, num_results=20):\n",
    "        \"\"\"Perform a Google search and get the first `num_results` relevant links (Free method).\"\"\"\n",
    "        links = []\n",
    "        try:\n",
    "            \n",
    "            for url in search(search_query, num_results=num_results):#, stop=num_results, pause=2):\n",
    "                links.append(url)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during Google search: {e}\")\n",
    "        \n",
    "        return links\n",
    "        \n",
    "    def get_links_user_prompt(self, website, job_description):\n",
    "        user_prompt = f\"Here is a job description for a position at {website.url}:\\n\\n{job_description}\\n\\n\"\n",
    "        user_prompt += \"Please determine which pages on the company website are relevant to this job and its context, including job details (job responsibilities, Required Qualifications, preferred requirements, job title), \\\n",
    "                        related projects, teams, departments, research, and company news. Identify all relevant web links and return the full \\\n",
    "                        https URLs in JSON format. Do not include Terms of Service, Privacy Policy, email links, or unrelated pages.\\n\"\n",
    "        user_prompt += \"Links (some might be relative links):\\n\"\n",
    "        user_prompt += \"\\n\".join(website.links)\n",
    "\n",
    "\n",
    "        \n",
    "        # user_prompt = f\"Here is the list of links on the website of {website.url} - \"\n",
    "        # user_prompt += \"please decide which of these are relevant web links for a brochure about the company, respond with the full https URL in JSON format. \\\n",
    "        #                 Do not include Terms of Service, Privacy, email links.\\n\"\n",
    "        # user_prompt += \"Links (some might be relative links):\\n\"\n",
    "        # user_prompt += \"\\n\".join(website.links)\n",
    "        return user_prompt\n",
    "\n",
    "\n",
    "    def get_links(self, url, job_desc):\n",
    "        website = Website(url)\n",
    "        # completion: your task is completing this conversation\n",
    "        response = self.openai.chat.completions.create(\n",
    "            model=self.MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.link_system_prompt},\n",
    "                {\"role\": \"user\", \"content\": self.get_links_user_prompt(website, job_desc)}\n",
    "          ],\n",
    "            response_format={\"type\": \"json_object\"} # we tell OpenAI, we want Json object back in its response. OpenAI in its documentation recommend that it's still important that you mention in your prompt that a json response is required even if you specify format in this argument.\n",
    "        )\n",
    "        result = response.choices[0].message.content \n",
    "        # dot choices zero. So what's this about? Well, as it happens we can actually in the API request ask to have multiple variations if we want, \n",
    "        # if we wanted it to generate several possible variations of the response. And we haven't done that. So we're only going to get back one.\n",
    "        # Uh, and so those variations come back in the form of these choices. But we've only got one. So choices zero is getting us the one and the only choice of the response back.\n",
    "        return json.loads(result)  # we use json.loads to bring it back as JSON.\n",
    "\n",
    "\n",
    "    # get content of page and link. Go into links and collect content\n",
    "    # def get_all_details(self, url):\n",
    "    #     result = \"Landing page:\\n\"\n",
    "    #     result += Website(url).get_contents()\n",
    "    #     links = self.get_links(url)\n",
    "    #     print(\"Found links:\", links)\n",
    "    #     for link in links[\"links\"]:\n",
    "    #         result += f\"\\n\\n{link['type']}\\n\"\n",
    "    #         result += Website(link[\"url\"]).get_contents()\n",
    "    #     return result\n",
    "\n",
    "    \n",
    "    def get_all_details(self, url, job_desc, company_name, visited=None):\n",
    "        if self.visited is None:\n",
    "            self.visited = set()\n",
    "        elif url in self.visited: # Avoid revisiting the same page\n",
    "            return '',visited\n",
    "    \n",
    "        self.visited.add(url)\n",
    "        # import pdb;pdb.set_trace()\n",
    "        \n",
    "        self.result += f\"Page: {url}\\n\"\n",
    "        self.result += Website(url).get_contents()\n",
    "    \n",
    "        # Get all links from the current page\n",
    "        links = self.get_links(url, job_desc)\n",
    "    \n",
    "        for link in links.get(\"links\", []):\n",
    "            link_url = link[\"url\"]\n",
    "            print(link_url)\n",
    "            # Ensure the link belongs to the same company domain\n",
    "            if self.is_same_domain(url, link_url):  \n",
    "                self.result += f\"\\n\\n{link['type']}\\n\"\n",
    "                tmp, _ = self.get_all_details(link_url, job_desc, self.visited)  # Recursive call\n",
    "                self.result += tmp\n",
    "                \n",
    "                \n",
    "    \n",
    "        return self.result, self.visited\n",
    "    \n",
    "    def is_same_domain(self, base_url, target_url):\n",
    "        \"\"\"Helper function to check if the target_url is within the same company domain.\"\"\"\n",
    "        base_domain = urlparse(base_url).netloc\n",
    "        target_domain = urlparse(target_url).netloc\n",
    "        return base_domain == target_domain\n",
    "        \n",
    "    \n",
    "\n",
    "    def get_brochure_user_prompt(self,company_name, job_desc, url):\n",
    "        # user_prompt = f\"You are looking at a company called: {company_name}\\n\"\n",
    "        # user_prompt += f\"Here are the contents of its landing page and other relevant pages; use this information to build a short brochure of the company in markdown.\\n\"\n",
    "        # user_prompt += self.get_all_details(url)\n",
    "        user_prompt = f\"You are researching a company called: {company_name}.\\n\\n\"\n",
    "        user_prompt += \"Below are the contents of its landing page and other relevant pages. Your goal is to extract and analyze \\\n",
    "                        all information related to a specific job position to help the user understand its full context within the company.\\n\\n\"\n",
    "        \n",
    "        user_prompt += \"### **Instructions:**\\n\"\n",
    "        user_prompt += \"- Identify and extract all details about the job, including responsibilities, qualifications, and benefits.\\n\"\n",
    "        user_prompt += \"- Find information on the department, team, or projects this job is associated with.\\n\"\n",
    "        user_prompt += \"- Look for company initiatives, research, or ongoing work that may relate to this role.\\n\"\n",
    "        user_prompt += \"- Extract any relevant company culture, values, or work environment details that impact this position.\\n\"\n",
    "        user_prompt += \"- If available, include company news, innovations, or industry focus related to the job role.\\n\"\n",
    "        user_prompt += \"- Provide a structured and comprehensive response in Markdown format for clarity.\\n\"\n",
    "        user_prompt += \"- Only show the connections in job descriptions and the webpage content to see what are related things\\n\"\n",
    "        user_prompt += \"- focus on everything that is related to the job description especially the job title/position title\\n\"\n",
    "        user_prompt += \"- if you do not find the related content, just output no information found. Do not generate unnecessary info\\n\"\n",
    "        user_prompt += \"\\n### **Job & Company Information:**\\n\"\n",
    "        \n",
    "        print(\"Searching the company website\")\n",
    "        tmp, visited = self.get_all_details(url, job_desc, company_name)\n",
    "        user_prompt += tmp\n",
    "        # import pdb; pdb.set_trace()\n",
    "        # Google Search\n",
    "        search_term = self.generate_search_term(job_description)\n",
    "        search_query = f\"{search_term} {company_name}\"  # Append company name to the search term\n",
    "    \n",
    "        print(f\"Searching Google for: {search_query}\")\n",
    "        import pdb; pdb.set_trace()\n",
    "        links = self.google_search(search_query)\n",
    "        for link in links:\n",
    "            tmp, visited = self.get_all_details(link, job_desc, company_name, visited)\n",
    "            user_prompt += tmp\n",
    "\n",
    "        # Google Search\n",
    "        '''search_term = self.generate_search_term(job_description)\n",
    "        search_query = f\"{search_term} {company_name}\"  # Append company name to the search term\n",
    "    \n",
    "        print(f\"Searching Google for: {search_query}\")\n",
    "        links = google_search(search_query)\n",
    "        print(\"Found links:\", links)'''\n",
    "\n",
    "        user_prompt = user_prompt[:10_000] # Truncate if more than 10,000 characters, just in case\n",
    "        return user_prompt\n",
    "\n",
    "\n",
    "\n",
    "    def create_brochure(self, company_name, url, job_desc):\n",
    "        response = self.openai.chat.completions.create(\n",
    "            model=self.MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                {\"role\": \"user\", \"content\": self.get_brochure_user_prompt(company_name, job_desc, url)}\n",
    "              ],\n",
    "        )\n",
    "        result = response.choices[0].message.content\n",
    "        display(Markdown(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9290671a-c51c-49f9-a21d-3272f802f666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching the company website\n",
      "https://neuropathbhc.org/internships/\n",
      "https://neuropathbhc.org/internships/\n",
      "https://neuropathbhc.org/careers/\n",
      "https://neuropathbhc.org/careers/\n",
      "https://neuropathbhc.org/internships/\n",
      "https://neuropathbhc.org/mission-and-vision/\n",
      "https://neuropathbhc.org/internships/\n",
      "https://neuropathbhc.org/careers/\n",
      "https://neuropathbhc.org/mission-and-vision/\n",
      "https://neuropathbhc.org/new-services/\n",
      "https://neuropathbhc.org/internships/\n",
      "https://neuropathbhc.org/careers/\n",
      "https://neuropathbhc.org/new-services/\n",
      "https://neuropathbhc.org/resources/\n",
      "https://neuropathbhc.org/internships/\n",
      "https://neuropathbhc.org/careers/\n",
      "https://neuropathbhc.org/mission-and-vision/\n",
      "https://neuropathbhc.org/resources/\n",
      "https://neuropathbhc.org/areas-of-specialization/\n",
      "https://neuropathbhc.org/careers/\n",
      "https://neuropathbhc.org/internships/\n",
      "https://neuropathbhc.org/areas-of-specialization/\n",
      "https://neuropathbhc.org/new-services/\n",
      "https://neuropathbhc.org/resources/\n",
      "https://neuropathbhc.org/areas-of-specialization/\n",
      "https://neuropathbhc.org/new-services/\n",
      "https://neuropathbhc.org/new-services/\n",
      "https://neuropathbhc.org/areas-of-specialization/\n",
      "https://neuropathbhc.org/approach/\n",
      "https://neuropathbhc.org/internships/\n",
      "https://neuropathbhc.org/careers/\n",
      "https://neuropathbhc.org/approach/\n",
      "https://neuropathbhc.org/mission-and-vision/\n",
      "https://neuropathbhc.org/areas-of-specialization/\n",
      "https://neuropathbhc.org/careers/\n",
      "https://neuropathbhc.org/areas-of-specialization/\n",
      "https://neuropathbhc.org/new-services/\n",
      "https://neuropathbhc.org/#book-consulting-services\n",
      "https://neuropathbhc.org/internships/\n",
      "https://neuropathbhc.org/careers/\n",
      "https://neuropathbhc.org/#book-consulting-services\n",
      "https://neuropathbhc.org/areas-of-specialization/\n",
      "https://neuropathbhc.org/new-services/\n",
      "Searching Google for: \"AI Intern machine learning projects site:linkedin.com OR site:indeed.com OR site:glassdoor.com\" Neuropath behavioral health\n",
      "Error during Google search: name 'search' is not defined\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```markdown\n",
       "## Job Position Context at Neuropath Behavioral Healthcare\n",
       "\n",
       "### Job Responsibilities and Qualifications\n",
       "- **Internship Programs**: Neuropath Behavioral Healthcare offers various internships focused on:\n",
       "  - Clinical Mental Health Counseling\n",
       "  - Behavioral Technician roles\n",
       "  - Behavioral Assistant roles\n",
       "  - Clinical Program Coordinator roles\n",
       "  - Client Services Administrator roles\n",
       "  - Healthcare Marketing and Digital Marketing Internships\n",
       "\n",
       "- **Key Responsibilities**:\n",
       "  - Interns provide **direct client interaction** and **supervised experience** alongside licensed therapists.\n",
       "  - Development of **counseling techniques** and **case conceptualization skills**.\n",
       "  - Participation in **individual and group therapy** sessions.\n",
       "  - Conducting **intakes**, **mental health screenings**, and **risk assessments**.\n",
       "  - Implementing **evidence-based treatment plans** and maintaining accurate clinical documentation.\n",
       "  - Engagement in **crisis intervention** and **interdisciplinary collaboration**.\n",
       "\n",
       "- **Learning Objectives** include:\n",
       "  - Understanding clinical assessment tools (DSM-5).\n",
       "  - Gaining experience in ethical and legal considerations in mental health care.\n",
       "  - Mastering therapeutic techniques such as **CBT**, **DBT**, and trauma-informed care.\n",
       "  - Developing competencies in **communication**, **social skills training**, and **client rapport**.\n",
       "\n",
       "### Department and Team\n",
       "- **Departmental Structure**:\n",
       "  - Behavioral Health Admin / Operations \n",
       "  - Client Services / Operations \n",
       "  - Field Operations for Therapeutic Supports and Respite Care \n",
       "  - Marketing and IT departments \n",
       "  - Human Resources / Operations\n",
       "\n",
       "- **Team Environment**: The company emphasizes a supportive work culture that encourages growth, mentorship, and interdisciplinary teamwork, with management respecting input from all levels.\n",
       "\n",
       "### Company Initiatives and Focus Areas\n",
       "- **Comprehensive Therapeutic Support Services** target a wide range of emotional, behavioral, and developmental challenges.\n",
       "- Services include:\n",
       "  - **Crisis Stabilization** and **Substance Abuse Recovery**, addressing various mental health disorders.\n",
       "  - **Telehealth**, **Outpatient Counseling**, and **In-home services**, particularly for children.\n",
       "  - Focus on **co-occurring disorders**, recognizing the necessity to balance mental health and substance use treatment.\n",
       "\n",
       "### Company Culture and Values\n",
       "- **Mission**: Empowering individuals and families to achieve mental wellness through compassionate, individualized care.\n",
       "- **Vision**: Leading the behavioral healthcare field by creating a technology-empowered ecosystem that makes high-quality, accessible mental health support a reality for all.\n",
       "- **Employee Testimonials** highlight the encouraging workplace culture, supportive management, and a commitment to conscientious care for clients, especially children.\n",
       "\n",
       "### Company News and Innovations\n",
       "- **New Programs**: Upcoming plans for expanding into adult services and addressing co-occurring disorders with comprehensive care approaches that include family support.\n",
       "- The emphasis on evidence-based practices reflects a commitment to staying at the forefront of mental health treatment.\n",
       "\n",
       "### Conclusion\n",
       "The internships and job roles at Neuropath Behavioral Healthcare focus on practical experience in a supportive and innovative environment. With a strong commitment to client care and development opportunities for staff, the organization provides a rich context for anyone looking to start or advance their career in the behavioral health field.\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# SA=SummarizationAgent()\n",
    "# SA.create_brochure(\"HuggingFace\", \"https://huggingface.com\") \n",
    "\n",
    "SA=SummarizationAgent()\n",
    "SA.create_brochure('Neuropath behavioral health',url, job_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0fa3e5-ce43-429a-baf4-b14e684fc903",
   "metadata": {},
   "source": [
    "Neuropath Behavioral Healthcare - Job Role Context\n",
    "Job Title: Internships\n",
    "Job Responsibilities:\n",
    "\n",
    "    Support activities designed to address emotional, behavioral, and developmental challenges in clients.\n",
    "    Participate in the implementation of evidence-based practices.\n",
    "    Assist in providing individualized care to clients under the supervision of trained professionals.\n",
    "    Engage in team meetings and training sessions to enhance learning and professional development.\n",
    "\n",
    "Qualifications:\n",
    "\n",
    "    An interest in mental health services and behavioral health therapy.\n",
    "    Pursuing or completed coursework relevant to psychology, social work, or related fields may be beneficial.\n",
    "    Strong communication skills and ability to work effectively in a team.\n",
    "\n",
    "Benefits:\n",
    "\n",
    "    Opportunity to gain hands-on experience in a therapeutic support environment.\n",
    "    Learn from experienced professionals in the mental health field.\n",
    "    Possible networking opportunities within the behavioral health community.\n",
    "\n",
    "Department and Team:\n",
    "\n",
    "    Internships fall under the Therapeutic Support Services department, contributing to individualized client care.\n",
    "    The role is part of a team of highly trained professionals dedicated to delivering personalized care.\n",
    "\n",
    "Company Initiatives and Projects:\n",
    "\n",
    "    Neuropath Behavioral Healthcare specializes in various services like outpatient counseling, telehealth, and crisis stabilization.\n",
    "    Focus on comprehensive therapeutic support to address a wide range of emotional and behavioral issues, which relates directly to the internship role.\n",
    "\n",
    "Company Culture and Values:\n",
    "\n",
    "    Neuropath promotes a supportive learning environment that emphasizes growth and professional development.\n",
    "    Employees report an encouraging environment where questions and input are valued, contributing to individual fulfillment and team cohesion.\n",
    "    The organization is recognized for its commitment to providing care that is often overlooked, particularly for children.\n",
    "\n",
    "Relevant Company News:\n",
    "\n",
    "    Neuropath is actively involved in expanding its services, including new programs for co-occurring disorders and enhanced in-home therapeutic offerings. This evolution aligns with ongoing intern projects aimed at improving service delivery.\n",
    "\n",
    "Conclusion:\n",
    "\n",
    "The internship position at Neuropath Behavioral Healthcare offers a comprehensive introduction to the world of behavioral health. Interns will engage with seasoned professionals while contributing to meaningful work that addresses the needs of clients facing a range of mental health challenges. This role not only supports the interns' educational goals but also reinforces the company's mission of compassion and evidence-based practice in mental health therapy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba153cc-7428-4e06-a17d-2e8dbf338c51",
   "metadata": {},
   "source": [
    "# Next implementation\n",
    "I used this implementation, but I found that the job description does not exist in the company website. \n",
    "\n",
    "This is the description of this implementation:\n",
    "\n",
    "`I want to write a code that collect all the information from webpages of a website, summarize each page in one paragraph,  and returns the related information to the job description. The web pages whose summary are related to the job description should be returned in a json format. This pages finallly, will be used to provide output for the user, as information on the company's website related to the job description. I do not want to do all of them together. Do these step by step. First, collecting web pages information, collecting summary of each page via openai-mini. second, use openai-mini to find the related pages based on collected summaries. third, returning the related pages' links. fourth, reading the content of links and create a markdown format of all information related to the job description. It can be summary or structured format that shows everything with a reference link in a table format or bullet point format`\n",
    "\n",
    "This implementation is not useful. It can not find anything on companies website, however, it is a deeper search. On the other side, it looks like the summary of pages does not help in finding the related information to the job description. So, the google search is more promising in this case because google does the job for us to find the related pages. The next implementation does it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "065f20a7-f2a8-4b8a-aa57-b5ed8001a0c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://www.zoetis.com/\n",
      "Fetching: https://www.zoetis.com/our-company/corporate-sustainability/advancing-sustainability-in-animal-health-for-a-better-future\n",
      "Fetching: https://www.zoetis.com/products-and-science/products/all-products?type=971538\n",
      "Fetching: https://www.zoetis.com/privacy-center\n",
      "Fetching: https://www.zoetis.com/products-and-science/products/revolution-plus\n",
      "Fetching: https://www.zoetis.com/products-and-science/petcare\n",
      "Fetching: https://www.zoetis.com/products-and-science/products/all-products?type=971536\n",
      "Fetching: https://www.zoetis.com/#page\n",
      "Fetching: https://www.zoetis.com/our-company/our-story/about-us\n",
      "Fetching: https://www.zoetis.com/#site-header-search\n",
      "Fetching: https://www.zoetis.com/products-and-science/products/cytopoint\n",
      "Fetching: https://www.zoetis.com/our-company/our-story/executive-team/\n",
      "Fetching: https://www.zoetis.com/join-us/\n",
      "Fetching: https://www.zoetis.com/products-and-science/products/librela\n",
      "Fetching: https://www.zoetis.com/customer-care#get-in-touch\n",
      "Fetching: https://www.zoetis.com/join-us/careers-at-zoetis\n",
      "Fetching: https://www.zoetis.com/our-company/zoetis-foundation/\n",
      "Fetching: https://www.zoetis.com/products-and-science/products/all-products\n",
      "Fetching: https://www.zoetis.com/news-and-insights/innovators-for-animals\n",
      "Fetching: https://www.zoetis.com/products-and-science/products/apoquel\n",
      "Fetching: https://www.zoetis.com/our-company/our-story/board-of-directors/\n",
      "Fetching: https://www.zoetis.com/join-us/life-at-zoetis\n",
      "Fetching: https://www.zoetis.com/our-company/corporate-compliance\n",
      "Fetching: https://www.zoetis.com/news-and-insights/news-insights\n",
      "Fetching: https://www.zoetis.com/join-us/intern-at-zoetis\n",
      "Fetching: https://www.zoetis.com/customer-care/\n",
      "Fetching: https://www.zoetis.com/our-company/awards-and-recognition/recent-awards-recognition\n",
      "Fetching: https://www.zoetis.com/products-and-science/products/all-products?type=971534\n",
      "Fetching: https://www.zoetis.com/join-us/diversity-equity-inclusion\n",
      "Fetching: https://www.zoetis.com/news-and-insights/blog/transforming-drug-discovery-and-development-with-generative-ai\n",
      "Fetching: https://www.zoetis.com/products-and-science/products/solensia\n",
      "Fetching: https://www.zoetis.com/our-company/cmo\n",
      "Fetching: https://www.zoetis.com/products-and-science/livestock\n",
      "Fetching: https://www.zoetis.com/news-and-insights/media-kit\n",
      "Fetching: https://www.zoetis.com/#headernavsiteselector\n",
      "Fetching: https://www.zoetis.com/contact-us\n",
      "Fetching: https://www.zoetis.com/products-and-science/equine\n",
      "Fetching: https://www.zoetis.com/news-and-insights/on-purpose/on-purpose-with-kristin-peck\n",
      "Fetching: https://www.zoetis.com/products-and-science/products/all-products?type=971533\n",
      "Fetching: https://www.zoetis.com/products-and-science/manufacturing-supply\n",
      "Fetching: https://www.zoetis.com/products-and-science/products/all-products?type=971541\n",
      "Fetching: https://www.zoetis.com/products-and-science/products/all-products?type=971540\n",
      "Fetching: https://www.zoetis.com/products-and-science/products/draxxin-kp\n",
      "Fetching: https://www.zoetis.com/terms-of-use\n",
      "Fetching: https://www.zoetis.com/products-and-science/diagnostics\n",
      "Fetching: https://www.zoetis.com/products-and-science/products/all-products?type=971539\n",
      "Fetching: https://www.zoetis.com/products-and-science/compound-transfer-program\n",
      "Fetching: https://www.zoetis.com/products-and-science/products/all-products?type=971535\n",
      "Fetching: https://www.zoetis.com/our-company/corporate-governance\n",
      "Fetching: https://www.zoetis.com/products-and-science/products/simparica-trio\n",
      "Fetching: https://www.zoetis.com/our-company/our-locations\n",
      "Fetching: https://www.zoetis.com/products-and-science/products/all-products?type=971531\n",
      "Fetching: https://www.zoetis.com/suppliers\n",
      "Fetching: https://www.zoetis.com/news-and-insights/feature-story-archive\n",
      "Fetching: https://www.zoetis.com/products-and-science/research-development\n",
      "Fetching: https://www.zoetis.com/products-and-science/partnering-with-zoetis\n",
      "Webpage collection complete!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from urllib.parse import urlparse\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class WebScraper:\n",
    "    def __init__(self):\n",
    "        self.visited = set()\n",
    "        self.pages = {}\n",
    "\n",
    "    def is_same_domain(self, base_url, target_url):\n",
    "        \"\"\"Check if the target URL belongs to the same domain as base_url.\"\"\"\n",
    "        base_domain = urlparse(base_url).netloc\n",
    "        target_domain = urlparse(target_url).netloc\n",
    "        return base_domain == target_domain\n",
    "\n",
    "    def get_links(self, url):\n",
    "        \"\"\"Extract all internal links from a webpage.\"\"\"\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            links = set()\n",
    "\n",
    "            for a_tag in soup.find_all(\"a\", href=True):\n",
    "                link = a_tag[\"href\"]\n",
    "                full_url = requests.compat.urljoin(url, link)\n",
    "                \n",
    "                if self.is_same_domain(url, full_url):\n",
    "                    links.add(full_url)\n",
    "\n",
    "            return list(links)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching links from {url}: {e}\")\n",
    "            return []\n",
    "\n",
    "    def get_page_content(self, url):\n",
    "        \"\"\"Extract visible text content from a webpage.\"\"\"\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            for script in soup([\"script\", \"style\"]):\n",
    "                script.extract()\n",
    "\n",
    "            text = soup.get_text(separator=\" \", strip=True)\n",
    "            return text[:5000]  # Limit content length to avoid too much data\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching content from {url}: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def crawl_website(self, url, depth=2):\n",
    "        \"\"\"Recursively crawl the website, collecting content from all pages.\"\"\"\n",
    "        if url in self.visited or depth == 0:\n",
    "            return\n",
    "\n",
    "        print(f\"Fetching: {url}\")\n",
    "        self.visited.add(url)\n",
    "        page_content = self.get_page_content(url)\n",
    "        if page_content:\n",
    "            self.pages[url] = page_content\n",
    "\n",
    "        links = self.get_links(url)\n",
    "        time.sleep(1)  # Delay to avoid overloading the server\n",
    "\n",
    "        for link in links:\n",
    "            self.crawl_website(link, depth - 1)\n",
    "\n",
    "        return self.pages  # Return dictionary of {url: content}\n",
    "\n",
    "\n",
    "# Usage\n",
    "scraper = WebScraper()\n",
    "company_website = \"https://www.zoetis.com/\"  # Replace with actual company website\n",
    "collected_pages = scraper.crawl_website(company_website)\n",
    "\n",
    "# Save collected pages for next steps\n",
    "import json\n",
    "with open(\"collected_pages.json\", \"w\") as f:\n",
    "    json.dump(collected_pages, f, indent=4)\n",
    "\n",
    "print(\"Webpage collection complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcf05ec5-6c37-44db-b211-2ab956b2255d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing: https://www.zoetis.com/\n",
      "Summarizing: https://www.zoetis.com/our-company/corporate-sustainability/advancing-sustainability-in-animal-health-for-a-better-future\n",
      "Summarizing: https://www.zoetis.com/products-and-science/products/all-products?type=971538\n",
      "Summarizing: https://www.zoetis.com/privacy-center\n",
      "Summarizing: https://www.zoetis.com/products-and-science/products/revolution-plus\n",
      "Summarizing: https://www.zoetis.com/products-and-science/petcare\n",
      "Summarizing: https://www.zoetis.com/products-and-science/products/all-products?type=971536\n",
      "Summarizing: https://www.zoetis.com/#page\n",
      "Summarizing: https://www.zoetis.com/our-company/our-story/about-us\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url, content \u001b[38;5;129;01min\u001b[39;00m collected_pages\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSummarizing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 48\u001b[0m     summary \u001b[38;5;241m=\u001b[39m \u001b[43msummarize_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     summarized_pages[url] \u001b[38;5;241m=\u001b[39m summary\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Save summaries for the next step\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 26\u001b[0m, in \u001b[0;36msummarize_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     22\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSummarize the following webpage content in one concise paragraph:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtext[:\u001b[38;5;241m4000\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Truncate if too long\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 26\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODEL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are an expert at summarizing website content.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m                  \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# response_format={\"type\": \"json_object\"} # we tell OpenAI, we want Json object back in its response. OpenAI in its documentation recommend that it's still important that you mention in your prompt that a json response is required even if you specify format in this argument.\u001b[39;49;00m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# response = openai.chat.completions.create(\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m#     model=MODEL,  # Use a lightweight model for efficiency\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m#     messages=[{\"role\": \"system\", \"content\": \"You are an expert at summarizing website content.\"},\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m#               {\"role\": \"user\", \"content\": prompt}],\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m#     max_tokens=150\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent \u001b[38;5;66;03m#response[\"choices\"][0][\"message\"][\"content\"].strip()\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/llms/lib/python3.11/site-packages/openai/_utils/_utils.py:279\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/llms/lib/python3.11/site-packages/openai/resources/chat/completions.py:859\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    856\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    857\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    858\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/llms/lib/python3.11/site-packages/openai/_base_client.py:1283\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1271\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1278\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1279\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1280\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1281\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1282\u001b[0m     )\n\u001b[0;32m-> 1283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniforge3/envs/llms/lib/python3.11/site-packages/openai/_base_client.py:960\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    958\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 960\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/llms/lib/python3.11/site-packages/openai/_base_client.py:996\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m    993\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending HTTP Request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, request\u001b[38;5;241m.\u001b[39mmethod, request\u001b[38;5;241m.\u001b[39murl)\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 996\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    997\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    998\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    999\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1000\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m   1002\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniforge3/envs/llms/lib/python3.11/site-packages/httpx/_client.py:926\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[1;32m    924\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 926\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/miniforge3/envs/llms/lib/python3.11/site-packages/httpx/_client.py:954\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    951\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 954\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    959\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    960\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/llms/lib/python3.11/site-packages/httpx/_client.py:991\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    989\u001b[0m     hook(request)\n\u001b[0;32m--> 991\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    993\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/miniforge3/envs/llms/lib/python3.11/site-packages/httpx/_client.py:1027\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1023\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1024\u001b[0m     )\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1027\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1031\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m~/miniforge3/envs/llms/lib/python3.11/site-packages/httpx/_transports/default.py:236\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    223\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    224\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    225\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    233\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    234\u001b[0m )\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 236\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    241\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    242\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    243\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    244\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    245\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/llms/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:256\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    253\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[0;32m~/miniforge3/envs/llms/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:236\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    232\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m~/miniforge3/envs/llms/lib/python3.11/site-packages/httpcore/_sync/connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/llms/lib/python3.11/site-packages/httpcore/_sync/http11.py:136\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/miniforge3/envs/llms/lib/python3.11/site-packages/httpcore/_sync/http11.py:106\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m     99\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    100\u001b[0m     (\n\u001b[1;32m    101\u001b[0m         http_version,\n\u001b[1;32m    102\u001b[0m         status,\n\u001b[1;32m    103\u001b[0m         reason_phrase,\n\u001b[1;32m    104\u001b[0m         headers,\n\u001b[1;32m    105\u001b[0m         trailing_data,\n\u001b[0;32m--> 106\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m     )\n\u001b[1;32m    114\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m~/miniforge3/envs/llms/lib/python3.11/site-packages/httpcore/_sync/http11.py:177\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    174\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/llms/lib/python3.11/site-packages/httpcore/_sync/http11.py:217\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    214\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 217\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/miniforge3/envs/llms/lib/python3.11/site-packages/httpcore/_backends/sync.py:128\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/llms/lib/python3.11/ssl.py:1295\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1292\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1293\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1294\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[0;32m~/miniforge3/envs/llms/lib/python3.11/ssl.py:1168\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1166\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1168\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[1;32m   1170\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# api_key = os.getenv('OPENAI_API_KEY')\n",
    "MODEL = 'gpt-4o-mini'\n",
    "openai = OpenAI()\n",
    "# Load collected web pages\n",
    "with open(\"collected_pages.json\", \"r\") as f:\n",
    "    collected_pages = json.load(f)\n",
    "\n",
    "# OpenAI API Configuration (Replace with your API key)\n",
    "# openai.api_key = \"your-api-key\"\n",
    "\n",
    "def summarize_text(text):\n",
    "    \"\"\"Use OpenAI-mini to summarize a webpage's content.\"\"\"\n",
    "    prompt = f\"Summarize the following webpage content in one concise paragraph:\\n\\n{text[:4000]}\"  # Truncate if too long\n",
    "\n",
    "    try:\n",
    "\n",
    "        response = openai.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are an expert at summarizing website content.\"},\n",
    "                      {\"role\": \"user\", \"content\": prompt}],\n",
    "        # response_format={\"type\": \"json_object\"} # we tell OpenAI, we want Json object back in its response. OpenAI in its documentation recommend that it's still important that you mention in your prompt that a json response is required even if you specify format in this argument.\n",
    "        )\n",
    "        \n",
    "        # response = openai.chat.completions.create(\n",
    "        #     model=MODEL,  # Use a lightweight model for efficiency\n",
    "        #     messages=[{\"role\": \"system\", \"content\": \"You are an expert at summarizing website content.\"},\n",
    "        #               {\"role\": \"user\", \"content\": prompt}],\n",
    "        #     max_tokens=150\n",
    "        # )\n",
    "        return response.choices[0].message.content #response[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error summarizing text: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Summarize each webpage\n",
    "summarized_pages = {}\n",
    "for url, content in collected_pages.items():\n",
    "    print(f\"Summarizing: {url}\")\n",
    "    summary = summarize_text(content)\n",
    "    summarized_pages[url] = summary\n",
    "\n",
    "# Save summaries for the next step\n",
    "with open(\"summarized_pages.json\", \"w\") as f:\n",
    "    json.dump(summarized_pages, f, indent=4)\n",
    "\n",
    "print(\"Summarization complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f858d9df-d24c-4c50-a104-134c432c978a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant pages found: 0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# api_key = os.getenv('OPENAI_API_KEY')\n",
    "MODEL = 'gpt-4o-mini'\n",
    "openai = OpenAI()\n",
    "\n",
    "# Load summarized web pages\n",
    "with open(\"collected_pages.json\", \"r\") as f:\n",
    "    summarized_pages = json.load(f)\n",
    "\n",
    "# Load job description\n",
    "# job_description = \"\"\"\n",
    "# Paste the job description here.\n",
    "# \"\"\"  # Replace with the actual job description\n",
    "\n",
    "# OpenAI API Configuration (Replace with your API key)\n",
    "# openai.api_key = \"your-api-key\"\n",
    "\n",
    "def is_relevant(summary, job_description):\n",
    "    \"\"\"Use OpenAI-mini to check if a webpage summary is relevant to the job description.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    The following is a job description:\n",
    "    {job_description}\n",
    "\n",
    "    Below is a summary of a webpage:\n",
    "    {summary}\n",
    "\n",
    "    Does this webpage contain information relevant to the job description? \n",
    "    Answer with 'yes' or 'no' and explain briefly why.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "\n",
    "        response = openai.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are an expert in analyzing job descriptions and company information.\"},\n",
    "                      {\"role\": \"user\", \"content\": prompt}],\n",
    "        )\n",
    "        result = response.choices[0].message.content \n",
    "        return \"yes\" in result  # If OpenAI responds with \"yes\", it's relevant\n",
    "    except Exception as e:\n",
    "        print(f\"Error determining relevance: {e}\")\n",
    "        return False\n",
    "\n",
    "# Filter relevant pages\n",
    "relevant_pages = {url: summary for url, summary in summarized_pages.items() if is_relevant(summary, job_description)}\n",
    "\n",
    "# Save relevant pages\n",
    "with open(\"relevant_pages.json\", \"w\") as f:\n",
    "    json.dump(relevant_pages, f, indent=4)\n",
    "\n",
    "print(f\"Relevant pages found: {len(relevant_pages)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b063c0a-bf82-4661-8c84-1ad2b5c1ddcd",
   "metadata": {},
   "source": [
    "# Using google "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "33e7872e-ec04-4afb-857e-4ad2f6c2afc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating search term...\n",
      "Searching Google for: \"Zoetis Precision Animal Health generative AI internship news projects\" Zoetis\n",
      "Processing: https://www.zoetis.com/news-and-insights/blog/transforming-drug-discovery-and-development-with-generative-ai\n",
      "Processing: https://zoetis.wd5.myworkdayjobs.com/en-US/zoetis/job/Precision-Animal-Health-Generative-AI-Intern_JR00016811-1\n",
      "Processing: https://www.linkedin.com/posts/rodrigo-carranza-2a66007b_precision-animal-health-generative-ai-intern-activity-7283004798772125696-ixzV\n",
      "Processing: https://www.linkedin.com/posts/di-liang_precision-animal-health-generative-ai-intern-activity-7267203648361766914-Eu5Y\n",
      "Processing: https://www.zoetis.com/join-us/intern-at-zoetis\n",
      "Processing: https://www.ziprecruiter.com/co/zoetis/Jobs/Research-Intern\n",
      "Processing: https://www.indeed.com/cmp/Zoetis/jobs\n",
      "Processing: https://zoetis.wd5.myworkdayjobs.com/en-US/zoetis/job/Organic-Chemistry-Process-Scientist_JR00016782-1\n",
      "Processing: https://www.zoetis.com/\n",
      "Processing: https://www.bcg.com/publications/2021/transforming-the-animal-health-industry\n",
      "Processing: https://builtin.com/job/third-party-risk-management-intern/3767239\n",
      "Processing: https://www.zoetis.com/news-and-insights/innovators-for-animals\n",
      "Processing: https://onehealthdev.org/internship-opportunities-at-zoetis-inc/\n",
      "Processing: https://www.facebook.com/katielynn444/\n",
      "Processing: https://www.zoetisus.com/news-and-media/\n",
      "Processing: https://www.glassdoor.com/Jobs/Zoetis-Jobs-E680848.htm\n",
      "Processing: https://www.zippia.com/ferring-pharmaceuticals-careers-23236/jobs/internship-jobs/\n",
      "Processing: https://www.zoetis.com/products-and-science/research-development\n",
      "Processing: https://www.instagram.com/tihoneurologists/?__d=1utm_sourceig_embed\n",
      "Processing: https://www.fieracapital.com/wp-content/uploads/default/20240827/strategy-profiles-and-results-2024-06-30.pdf\n",
      "Summarization complete! Results saved to google_search_summaries.json.\n"
     ]
    }
   ],
   "source": [
    "# !pip install googlesearch-python\n",
    "import openai\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from googlesearch import search  # Free Google search library\n",
    "\n",
    "# OpenAI API Configuration (Replace with your API key)\n",
    "# OPENAI_API_KEY = \"your-openai-key\"  # Replace with your OpenAI API key\n",
    "# openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "def generate_search_term(job_description):\n",
    "    \"\"\"Use OpenAI to generate an optimized search term based on the job description.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Given the following job description, generate the best search term to find relevant information about this role, \n",
    "    including company projects, industry focus, and related news. The search term should be concise and effective for Google search.\n",
    "\n",
    "    Job Description:\n",
    "    {job_description}\n",
    "\n",
    "    Provide only the search term without explanation.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response=openai.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[{\"role\": \"system\", \"content\": \"You are an expert at creating effective Google search queries.\"},\n",
    "                          {\"role\": \"user\", \"content\": prompt}],\n",
    "        )\n",
    "        # response = openai.chat.completions.create(\n",
    "        #     model=MODEL,\n",
    "        #     messages=[{\"role\": \"system\", \"content\": \"You are an expert at creating effective Google search queries.\"},\n",
    "        #               {\"role\": \"user\", \"content\": prompt}],\n",
    "        #     # max_tokens=20\n",
    "        # )\n",
    "        return response.choices[0].message.content \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating search term: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def google_search(search_query, num_results=20):\n",
    "    \"\"\"Perform a Google search and get the first `num_results` relevant links (Free method).\"\"\"\n",
    "    links = []\n",
    "    try:\n",
    "        \n",
    "        for url in search(search_query, num_results=num_results):#, stop=num_results, pause=2):\n",
    "            links.append(url)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Google search: {e}\")\n",
    "    \n",
    "    return links\n",
    "\n",
    "def extract_page_text(url):\n",
    "    \"\"\"Extract main content from a webpage (ignoring scripts and styles).\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()\n",
    "\n",
    "        text = soup.get_text(separator=\" \", strip=True)\n",
    "        return text[:5000]  # Limit content to avoid excessive text\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching content from {url}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def summarize_text(text):\n",
    "    \"\"\"Use OpenAI-mini to summarize a webpage's content.\"\"\"\n",
    "    prompt = f\"Summarize the following webpage content in one concise paragraph:\\n\\n{text[:4000]}\"  # Truncate if too long\n",
    "\n",
    "    try:\n",
    "        response=openai.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[{\"role\": \"system\", \"content\": \"You are an expert at summarizing website content.\"},\n",
    "                      {\"role\": \"user\", \"content\": prompt}],\n",
    "        )\n",
    "        # response = openai.chat.completions.create(\n",
    "        #     model=MODEL,\n",
    "        #     messages=[{\"role\": \"system\", \"content\": \"You are an expert at summarizing website content.\"},\n",
    "        #               {\"role\": \"user\", \"content\": prompt}],\n",
    "        #     # max_tokens=150\n",
    "        # )\n",
    "        return response.choices[0].message.content \n",
    "    except Exception as e:\n",
    "        print(f\"Error summarizing text: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def main(job_description, company_name):\n",
    "    \"\"\"Main function to generate a search term, search Google, extract page summaries, and store results.\"\"\"\n",
    "    print(\"Generating search term...\")\n",
    "    search_term = generate_search_term(job_description)\n",
    "    search_query = f\"{search_term} {company_name}\"  # Append company name to the search term\n",
    "\n",
    "    print(f\"Searching Google for: {search_query}\")\n",
    "    links = google_search(search_query)\n",
    "\n",
    "    summarized_pages = {}\n",
    "    \n",
    "    for url in links:\n",
    "        print(f\"Processing: {url}\")\n",
    "        page_content = extract_page_text(url)\n",
    "        \n",
    "        if page_content:\n",
    "            summary = summarize_text(page_content)\n",
    "            summarized_pages[url] = summary\n",
    "        \n",
    "        time.sleep(1)  # Delay to avoid overwhelming the servers\n",
    "\n",
    "    # Save results\n",
    "    with open(\"google_search_summaries.json\", \"w\") as f:\n",
    "        json.dump(summarized_pages, f, indent=4)\n",
    "\n",
    "    print(\"Summarization complete! Results saved to google_search_summaries.json.\")\n",
    "\n",
    "# Example Usage\n",
    "# job_description = \"\"\"\n",
    "# Paste the job description here.\n",
    "# \"\"\"  # Replace with actual job description\n",
    "company_name = \"Zoetis\"\n",
    "main(job_description, company_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ee659c-9e8b-46fa-bb35-39c591e8c961",
   "metadata": {},
   "source": [
    "# Next step\n",
    "After searching for the job on google, add urls of relevant urls to the previous implementation (AGENT) and ask the Agent to use it for incorporating info to the output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
