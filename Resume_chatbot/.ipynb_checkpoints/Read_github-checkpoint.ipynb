{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from github import Github\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "ACCESS_TOKEN = os.getenv(\"GITHUB_TOKEN\")\n",
    "api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "openai = OpenAI(api_key=api_key)\n",
    "def generate_code_summary(code):\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that summarizes the projects of a github repositories. If you are not able to summarize the code, just return 'No summary available'. If the content of file is not readme file or code related to the project, just return 'No summary available'\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Summarize this code or text:\\n{code[:3000]}. If you are not able to summarize the code, just return 'No summary available'. If the content of file is not readme file or code related to the project, just return 'No summary available'\"}\n",
    "            ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def generate_project_summary(file_summaries):\n",
    "    summaries = \"\\n\".join(file_summaries)\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an assistant that summarizes the projects of a github repositories. You will be given a list of file summaries. You need to summarize the projects of the repository based on the file summaries. If you are not able to summarize the code, just return 'No summary available'. If the content of file is not readme file or code related to the project, just return 'No summary available'\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Here are the file summaries: {summaries}\"}\n",
    "            ]\n",
    "    )\n",
    "    output = response.choices[0].message.content\n",
    "    print(\"-\"*100)\n",
    "    print(output)\n",
    "    print(\"-\"*100)\n",
    "    return output\n",
    "\n",
    "def save_repo_report(repo_name, content):\n",
    "    with open(f\"{repo_name}_report.txt\", \"w\") as f:\n",
    "        f.write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarizing all repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_repositories():\n",
    "    g = Github(ACCESS_TOKEN)\n",
    "    user = g.get_user()\n",
    "    \n",
    "    for repo in user.get_repos():\n",
    "        if \n",
    "        print(f\"Analyzing {repo.full_name}\")\n",
    "        contents = repo.get_contents(\"\")\n",
    "        file_summaries = []\n",
    "        # print(f\"Contents: {contents}\")\n",
    "        \n",
    "        \n",
    "        while contents:\n",
    "            file_content = contents.pop(0)\n",
    "            if file_content.type == \"dir\":\n",
    "                contents.extend(repo.get_contents(file_content.path))\n",
    "            else:\n",
    "                print(f\"File: {file_content.path}\")\n",
    "                if file_content.size < 1000000:  # Avoid large files\n",
    "                    try:\n",
    "                        content = file_content.decoded_content.decode()\n",
    "                        summary = generate_code_summary(content)\n",
    "                        if summary !='No summary available':\n",
    "                                file_summaries.append(f\"{file_content.path}: {summary}\")\n",
    "                                \n",
    "                    except Exception as e:\n",
    "                        continue\n",
    "        if len(file_summaries) > 0:\n",
    "            project_summary = generate_project_summary(file_summaries)\n",
    "            save_repo_report(repo.name, project_summary)\n",
    "\n",
    "analyze_repositories()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarizing one repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing mortezamg63/YOLO-Object-Detecton\n",
      "File: README.md\n",
      "File: OpenCV Implementation/main.py\n",
      "File: OpenCV Implementation/settings.py\n",
      "File: OpenCV Implementation/utils.py\n",
      "File: TensorFlow implementation/convert_weights.py\n",
      "File: TensorFlow implementation/convert_weights_pb.py\n",
      "File: TensorFlow implementation/main.py\n",
      "File: TensorFlow implementation/settings.py\n",
      "File: TensorFlow implementation/utils.py\n",
      "File: TensorFlow implementation/yolo_v3.py\n",
      "File: TensorFlow implementation/yolo_v3_tiny.py\n",
      "File: OpenCV Implementation/extracted_objects/frame15_ExtObj_1.jpg\n",
      "File: OpenCV Implementation/extracted_objects/frame17_ExtObj_2.jpg\n",
      "File: OpenCV Implementation/extracted_objects/frame3_ExtObj_0.jpg\n",
      "File: TensorFlow implementation/extracted_regions/frame15_ExtObj_1.jpg\n",
      "File: TensorFlow implementation/extracted_regions/frame1_ExtObj_0.jpg\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The project is focused on implementing a YOLO (You Only Look Once) object detection system using two key frameworks: TensorFlow and OpenCV. It supports both the full YOLO v3 model and the smaller YOLO v3 Tiny version. \n",
      "\n",
      "- With TensorFlow, the project includes scripts to convert model weights into formats suitable for TensorFlow execution (ckpt or pb), and utilizes GPU resources to perform object detection on video frames, saving detected objects as cropped images. TensorFlow scripts also offer the capability to run inference using a frozen graph or checkpoint model.\n",
      "\n",
      "- The OpenCV implementation concentrates on utilizing a pre-trained YOLO model for real-time object detection by leveraging the `dnn` module of OpenCV. This involves capturing video frames, processing them to detect objects, and subsequently displaying results with bounding boxes.\n",
      "\n",
      "Each implementation contains a main entry point script to execute the detection process based on configurations set in a separate settings file. Additionally, utilities are provided for network processing and bounding box management, ensuring robust object detection performance across various input media.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from github import Github\n",
    "\n",
    "def analyze_repository(repo_full_name):\n",
    "    g = Github(ACCESS_TOKEN)\n",
    "    try:\n",
    "        repo = g.get_repo(repo_full_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Analyzing {repo.full_name}\")\n",
    "    contents = repo.get_contents(\"\")\n",
    "    file_summaries = []\n",
    "\n",
    "    while contents:\n",
    "        file_content = contents.pop(0)\n",
    "        if file_content.type == \"dir\":\n",
    "            contents.extend(repo.get_contents(file_content.path))\n",
    "        else:\n",
    "            print(f\"File: {file_content.path}\")\n",
    "            if file_content.size < 1000000:  # Avoid large files\n",
    "                try:\n",
    "                    content = file_content.decoded_content.decode()\n",
    "                    summary = generate_code_summary(content)\n",
    "                    if summary != 'No summary available':\n",
    "                        file_summaries.append(f\"{file_content.path}: {summary}\")\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "    if len(file_summaries) > 0:\n",
    "        project_summary = generate_project_summary(file_summaries)\n",
    "        save_repo_report(repo.name, project_summary)\n",
    "\n",
    "# Example usage:\n",
    "repo_full_name = \"mortezamg63/YOLO-Object-Detecton\"  # Replace with your actual repo full name\n",
    "analyze_repository(repo_full_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Autoencoder_report.txt',\n",
       " 'awesome-long-tail-learning_report.txt',\n",
       " 'Read_github.ipynb',\n",
       " 'Accessing-and-modifying-different-layers-of-a-pretrained-model-in-pytorch_report.txt',\n",
       " 'Adaptive-Median-Filter_report.txt']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
