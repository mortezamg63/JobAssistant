{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b531cd78-a9a4-48fb-b7d6-c19902757220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from urllib.parse import urlparse\n",
    "from googlesearch import search  # Free Google search library\n",
    "# import validators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "358d78b4-07e4-4137-a42b-3ea8c2734212",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b852302d-e55c-425c-b8d4-ccbca33df1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Website:\n",
    "    headers = {\n",
    " \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "}\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        response = requests.get(url, headers=self.headers)\n",
    "        self.body = response.content\n",
    "        soup = BeautifulSoup(self.body, 'html.parser')\n",
    "        self.title = soup.title.string if soup.title else \"No title found\"\n",
    "        if soup.body:\n",
    "            for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "                irrelevant.decompose()\n",
    "            self.text = soup.body.get_text(separator=\"\\n\", strip=True)\n",
    "        else:\n",
    "            self.text = \"\"\n",
    "        links = [link.get('href') for link in soup.find_all('a')]\n",
    "        self.links = [link for link in links if link]\n",
    "\n",
    "    def get_contents(self):\n",
    "        return f\"Webpage Title:\\n{self.title}\\nWebpage Contents:\\n{self.text}\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbb023f7-f58a-47f6-9920-61bbafb4cfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://www.Etched.com/'\n",
    "job_description=\"\"\"    About Etched\n",
    "\n",
    "Etched is building AI chips that are hard-coded for individual model architectures. Our first product (Sohu) only supports transformers, but has an order of magnitude more throughput and lower latency than a B200. With Etched ASICs, you can build products that would be impossible with GPUs, like real-time video generation models and extremely deep chain-of-thought reasoning.\n",
    "\n",
    "Backed by $150M+ in funding from investors like Peter Thiel, Jane Street, and Two Sigma, we're scaling rapidly and looking for exceptional interns to join us in Summer 2025. Interns will get to work on cutting-edge engineering and business challenges across the following departments at Etched:\n",
    "\n",
    "Hardware Engineering (ASIC):\n",
    "\n",
    "Our Hardware team is designing next-generation silicon that runs transformer models faster than any chip in history. Led by Ajat Hukkoo (ex-Broadcom Distinguished Engineer, Intel VP) and Mark Ross (former Cypress CTO; shipped 5 different $1B+ silicon systems), our Hardware team is looking for excellent electrical and computer engineers to help us design and verify our next-gen silicon.\n",
    "\n",
    "Architecture:\n",
    "\n",
    "Our Architecture team, led by Saptadeep Pal (Co-founder/Chief Architect at Auradine, ex-NVIDIA architecture team), is designing the underlying chip architecture for our silicon. Architecture interns will help solve novel challenges in microarchitecture design to drive unprecedented FLOPS on our chip.\n",
    "\n",
    "Platform Engineering:\n",
    "\n",
    "Our Platform team is led by Brian Loiler, NVIDIA's former Senior Director of Datacenter Systems Engineering who built multiple generations of DGX and HGX. Scaling from single chips to full datacenter deployments requires solving complex challenges in thermal design, power delivery, system integration, and more. We're looking for exceptional electrical and mechanical engineers to join the Platform team and help us build state-of-the-art systems at scale.\n",
    "\n",
    "Software Engineering and Machine Learning:\n",
    "\n",
    "Our Software and Machine Learning teams build firmware, inference engines, and model infrastructure for Sohu. Our stack is engineered to optimize transformer performance - enabling capabilities that are impossible on general-purpose chips. The team is led by David Munday, who helped lead data center and TPU infrastructure at Google and DeepMind for 7 years. We're looking for leading SWEs and machine learning researchers across a variety of roles.\n",
    "\n",
    "Operations:\n",
    "\n",
    "The operations team is our special forces unit, working on the company's most critical business initiatives alongside our COO, Robert Wachen. We tackle problems through rigorous analysis, strategic planning, and relentless execution. We're looking for sharp, ambitious interns who can thrive under pressure, take complete ownership of complex problems, and deliver measurable results.\n",
    "\n",
    "Go-to-Market:\n",
    "\n",
    "The Go-to-Market team crafts the strategy and executes the plans to bring Sohu to market, helping customers leverage Sohu to build previously-impossible products. The team includes Chase Holmes, former top-seller at Databricks and MosaicML, and Alexandra Boyle, former CCO at Symphony, and operates under Robert, our COO who previously scaled Prod, an AI accelerator, to a $7B+ cohort valuation. We're looking for talented customer-facing interns to help accelerate our go-to-market engine.\n",
    "\n",
    "Talent Acquisition:\n",
    "\n",
    "The Talent team identifies and attracts the people who make up Etched. The team works directly with leadership to assemble world-class experts across all departments. We're looking for technical recruiting interns who can build relationships and source top talent.\n",
    "\n",
    "Program Details:\n",
    "\n",
    "12-week paid internship (June - August 2025)\n",
    "\n",
    "Generous housing support for those relocating\n",
    "\n",
    "Based at our office in San Jose, CA\n",
    "\n",
    "Direct mentorship from industry leaders and world-class engineers\n",
    "\n",
    "Opportunity to work on one of the most important problems of our time\n",
    "\n",
    "For any questions, contact internships@etched.com.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe36322e-4b28-478b-9a7d-1a464d5ca6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummarizationAgent:\n",
    "    def __init__(self):\n",
    "        self.link_system_prompt = \"You are provided with a list of links found on a webpage. \\\n",
    "        You are able to decide which of the links would be most relevant to include in a brochure about the company, \\\n",
    "        such as links to an About page, or a Company page, or Careers/Jobs pages.\\n\"\n",
    "        \n",
    "        self.link_system_prompt += \"You should respond in JSON as in this example:\"\n",
    "        self.link_system_prompt += \"\"\"\n",
    "        {\n",
    "            \"links\": [\n",
    "                {\"type\": \"about page\", \"url\": \"https://full.url/goes/here/about\"},\n",
    "                {\"type\": \"careers page\": \"url\": \"https://another.full.url/careers\"}\n",
    "            ]\n",
    "        }\n",
    "        \"\"\"\n",
    "\n",
    "        self.link_system_prompt = \"You are provided with a list of links found on a webpage. \\\n",
    "        Your task is to determine which links are most relevant for finding job-related information, \\\n",
    "        including job postings, team structures, related projects, departments, company research, and company news. \\\n",
    "        Prioritize pages that provide context about the job role and its industry impact.\\n\"\n",
    "        self.link_system_prompt += \"You should respond in JSON as in this example:\"\n",
    "        self.link_system_prompt += \"\"\"\n",
    "                {\n",
    "                    \"links\": [\n",
    "                        {\"type\": \"job posting\", \"url\": \"https://full.url/goes/here/job-title\"},\n",
    "                        {\"type\": \"careers page\", \"url\": \"https://full.url/goes/here/careers\"},\n",
    "                        {\"type\": \"team/department page\", \"url\": \"https://full.url/goes/here/team\"},\n",
    "                        {\"type\": \"project/research page\", \"url\": \"https://full.url/goes/here/project\"},\n",
    "                        {\"type\": \"company news\", \"url\": \"https://full.url/goes/here/news\"}\n",
    "                    ]\n",
    "                }\n",
    "        \"\"\"\n",
    "\n",
    "        self.system_prompt = \"You are an assistant that searches for and extracts all available information related to a job posting from a company website. \\\n",
    "                Your goal is to gather as much detail as possible to help the user understand the job role, its responsibilities, \\\n",
    "                the projects it is related to, the team or department it belongs to, and any background about the company’s work in this area. Find anything that can be related to keywords in job description\\\n",
    "                Analyze relevant webpages, including careers, company projects, teams, and news, to provide a comprehensive understanding \\\n",
    "                of the job in its full context. Preserve all relevant text and descriptions while ensuring clarity and completeness. \\\n",
    "                Respond in Markdown format.\"\n",
    "\n",
    "        load_dotenv(override=True)\n",
    "        api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "        self.MODEL = 'gpt-4o-mini'\n",
    "        self.openai = OpenAI()\n",
    "        \n",
    "\n",
    "    \n",
    "    def generate_search_term(self, job_description):\n",
    "        \"\"\"Use OpenAI to generate an optimized search term based on the job description.\"\"\"\n",
    "        # V2\n",
    "        prompt = f\"\"\"\n",
    "                    Given the following job description, generate an optimized search term to find **relevant background information** about this role. \n",
    "                    Avoid returning direct job postings or career listings that repeat the job description.\n",
    "                    \n",
    "                    The search term should help find:\n",
    "                    - **Company projects** related to this job role.\n",
    "                    - **Industry focus and research** relevant to this field.\n",
    "                    - **Technological advancements, trends, or recent developments** in this domain.\n",
    "                    - **Insights about the company’s work culture, leadership, and market positioning**.\n",
    "                    \n",
    "                    **Do NOT include keywords that return job listings or career pages**.\n",
    "                    \n",
    "                    ### Job Description:\n",
    "                    {job_description}\n",
    "                    \n",
    "                    Provide ONLY the search term without explanation.\n",
    "                    \"\"\"\n",
    "\n",
    "        # V1\n",
    "        # prompt = f\"\"\"\n",
    "        # Given the following job description, generate the best search term to find relevant information about this role, \n",
    "        # including company projects, industry focus, and related news. The search term should be concise and effective for Google search.\n",
    "    \n",
    "        # Job Description:\n",
    "        # {job_description}\n",
    "    \n",
    "        # Provide only the search term without explanation.\n",
    "        # \"\"\"\n",
    "    \n",
    "        try:\n",
    "            response=self.openai.chat.completions.create(\n",
    "                model=self.MODEL,\n",
    "                messages=[{\"role\": \"system\", \"content\": \"You are an expert at creating effective Google search queries.\"},\n",
    "                              {\"role\": \"user\", \"content\": prompt}],\n",
    "            )\n",
    "            # response = openai.chat.completions.create(\n",
    "            #     model=MODEL,\n",
    "            #     messages=[{\"role\": \"system\", \"content\": \"You are an expert at creating effective Google search queries.\"},\n",
    "            #               {\"role\": \"user\", \"content\": prompt}],\n",
    "            #     # max_tokens=20\n",
    "            # )\n",
    "            return response.choices[0].message.content \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating search term: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    \n",
    "    def check_url_reachable(self, url):\n",
    "        try:\n",
    "            response = requests.head(url, allow_redirects=True, timeout=5)  # HEAD is faster than GET\n",
    "            return response.status_code == 200\n",
    "        except requests.RequestException:\n",
    "            return False\n",
    "\n",
    "    def google_search(self, search_query, num_results=20):\n",
    "        \"\"\"Perform a Google search and get the first `num_results` relevant links (Free method).\"\"\"\n",
    "        links = []\n",
    "        try:\n",
    "            \n",
    "            for url in search(search_query, num_results=num_results):#, stop=num_results, pause=2):\n",
    "                if self.check_url_reachable(url):\n",
    "                    links.append(url)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during Google search: {e}\")\n",
    "        \n",
    "        return links\n",
    "        \n",
    "    def get_links_user_prompt(self, website, job_description):\n",
    "        user_prompt = f\"Here is a job description for a position at {website.url}:\\n\\n{job_description}\\n\\n\"\n",
    "        user_prompt += \"Please determine which pages on the company website are relevant to this job and its context, including job details (job responsibilities, Required Qualifications, preferred requirements, job title), \\\n",
    "                        related projects, teams, departments, research, and company news. Identify all relevant web links and return the full \\\n",
    "                        https URLs in JSON format. Do not include Terms of Service, Privacy Policy, email links, or unrelated pages.\\n\"\n",
    "        user_prompt += \"Links (some might be relative links):\\n\"\n",
    "        user_prompt += \"\\n\".join(website.links)\n",
    "\n",
    "\n",
    "        \n",
    "        # user_prompt = f\"Here is the list of links on the website of {website.url} - \"\n",
    "        # user_prompt += \"please decide which of these are relevant web links for a brochure about the company, respond with the full https URL in JSON format. \\\n",
    "        #                 Do not include Terms of Service, Privacy, email links.\\n\"\n",
    "        # user_prompt += \"Links (some might be relative links):\\n\"\n",
    "        # user_prompt += \"\\n\".join(website.links)\n",
    "        return user_prompt\n",
    "\n",
    "\n",
    "    def get_links(self, url, job_desc):\n",
    "        website = Website(url)\n",
    "        # completion: your task is completing this conversation\n",
    "        response = self.openai.chat.completions.create(\n",
    "            model=self.MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.link_system_prompt},\n",
    "                {\"role\": \"user\", \"content\": self.get_links_user_prompt(website, job_desc)}\n",
    "          ],\n",
    "            response_format={\"type\": \"json_object\"} # we tell OpenAI, we want Json object back in its response. OpenAI in its documentation recommend that it's still important that you mention in your prompt that a json response is required even if you specify format in this argument.\n",
    "        )\n",
    "        result = response.choices[0].message.content \n",
    "        # dot choices zero. So what's this about? Well, as it happens we can actually in the API request ask to have multiple variations if we want, \n",
    "        # if we wanted it to generate several possible variations of the response. And we haven't done that. So we're only going to get back one.\n",
    "        # Uh, and so those variations come back in the form of these choices. But we've only got one. So choices zero is getting us the one and the only choice of the response back.\n",
    "        return json.loads(result)  # we use json.loads to bring it back as JSON.\n",
    "\n",
    "\n",
    "    # get content of page and link. Go into links and collect content\n",
    "    # def get_all_details(self, url):\n",
    "    #     result = \"Landing page:\\n\"\n",
    "    #     result += Website(url).get_contents()\n",
    "    #     links = self.get_links(url)\n",
    "    #     print(\"Found links:\", links)\n",
    "    #     for link in links[\"links\"]:\n",
    "    #         result += f\"\\n\\n{link['type']}\\n\"\n",
    "    #         result += Website(link[\"url\"]).get_contents()\n",
    "    #     return result\n",
    "\n",
    "    \n",
    "    # def get_all_details(self, url, job_desc, company_name, visited=None):\n",
    "    #     if visited is None:\n",
    "    #         visited = set()\n",
    "    \n",
    "    #     # Avoid revisiting the same page\n",
    "    #     if url in visited:\n",
    "    #         return \"\"\n",
    "    \n",
    "    #     visited.add(url)\n",
    "    \n",
    "    #     result = f\"Page: {url}\\n\"\n",
    "    #     result += Website(url).get_contents()\n",
    "    \n",
    "    #     # Get all links from the current page\n",
    "    #     links = self.get_links(url, job_desc)\n",
    "    \n",
    "    #     for link in links.get(\"links\", []):\n",
    "    #         link_url = link[\"url\"]\n",
    "    \n",
    "    #         # Ensure the link belongs to the same company domain\n",
    "    #         if self.is_same_domain(url, link_url):  \n",
    "    #             result += f\"\\n\\n{link['type']}\\n\"\n",
    "    #             result += self.get_all_details(link_url, job_desc, visited)  # Recursive call\n",
    "    \n",
    "    #     return result, visited\n",
    "    \n",
    "    \n",
    "    # def is_same_domain(self, base_url, target_url):\n",
    "    #     \"\"\"Helper function to check if the target_url is within the same company domain.\"\"\"\n",
    "    #     base_domain = urlparse(base_url).netloc\n",
    "    #     target_domain = urlparse(target_url).netloc\n",
    "    #     return base_domain == target_domain\n",
    "    \n",
    "        \n",
    "    def get_all_details(self, url):\n",
    "        result = \"Landing page:\\n\"\n",
    "        result += Website(url).get_contents()\n",
    "        return result\n",
    "\n",
    "    def get_brochure_user_prompt(self,company_name, job_desc, url):\n",
    "        # user_prompt = f\"You are looking at a company called: {company_name}\\n\"\n",
    "        # user_prompt += f\"Here are the contents of its landing page and other relevant pages; use this information to build a short brochure of the company in markdown.\\n\"\n",
    "        # user_prompt += self.get_all_details(url)\n",
    "        user_prompt = f\"You are researching a company called: {company_name}.\\n\\n\"\n",
    "        user_prompt += \"Below are the contents of its landing page and other relevant pages. Your goal is to extract and analyze \\\n",
    "                        all information related to a specific job position to help the user understand its full context within the company.\\n\\n\"\n",
    "\n",
    "        \n",
    "        # Google Search\n",
    "        search_term = self.generate_search_term(job_description)\n",
    "        search_query = f\"{search_term} {company_name}\"  # Append company name to the search term\n",
    "    \n",
    "        print(f\"Searching Google for: {search_query}\")\n",
    "        links = self.google_search(search_query, num_results=20)\n",
    "        print(links)\n",
    "        for link in links:\n",
    "            user_prompt += self.get_all_details(link)\n",
    "            \n",
    "        user_prompt += \"### **Instructions:**\\n\"\n",
    "        # user_prompt += \"- Identify and extract all details about the job, including responsibilities, qualifications, and benefits.\\n\"\n",
    "        user_prompt += \"- Find information on the department, team, or projects this job is associated with.\\n\"\n",
    "        user_prompt += \"- Look for company initiatives, research, or ongoing work that may relate to this role.\\n\"\n",
    "        user_prompt += \"- Extract any relevant company culture, values, or work environment details that impact this position.\\n\"\n",
    "        user_prompt += \"- If available, include company news, innovations, or industry focus related to the job role.\\n\"\n",
    "        user_prompt += \"- Provide a structured and comprehensive response in Markdown format for clarity.\\n\"\n",
    "        user_prompt += \"- Only show the connections in job descriptions and the webpage content to see what are related things\\n\"\n",
    "        user_prompt += \"- focus on everything that is related to the job description especially the job title/position title\\n\"\n",
    "        # user_prompt += \"- if you do not find the related content, just output no information found. Do not generate unnecessary info\\n\"\n",
    "        user_prompt += \" provide as much information as you can. Also, do not include the job description in the output. Response in markdown format\\n\"\n",
    "        user_prompt += \"\\n### **Job & Company Information:**\\n\"\n",
    "\n",
    "\n",
    "        user_prompt = user_prompt[:10_000] # Truncate if more than 10,000 characters, just in case\n",
    "        return user_prompt\n",
    "\n",
    "\n",
    "    def create_brochure(self, company_name, url, job_desc):\n",
    "        response = self.openai.chat.completions.create(\n",
    "            model=self.MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                {\"role\": \"user\", \"content\": self.get_brochure_user_prompt(company_name, job_desc, url)}\n",
    "              ],\n",
    "        )\n",
    "        result = response.choices[0].message.content\n",
    "        display(Markdown(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883f7633-5dac-4e27-a5c9-a4e897ff54c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching Google for: \"Etched AI chips Sohu transformers projects technology trends company culture leadership market positioning\" Zoetis\n",
      "['https://www.etched.com/announcing-etched', 'https://techcrunch.com/2024/06/25/etched-is-building-an-ai-chip-that-only-runs-transformer-models/', 'https://medium.com/@maxel333/etched-sohu-revolutionizing-ai-with-transformer-specific-chips-4a8661394f49', 'https://aimresearch.co/market-industry/how-two-harvard-dropouts-are-taking-on-nvidia-with-a-chip-that-could-change-ai-forever', 'https://www.linkedin.com/pulse/specialized-ai-chips-etched-aim-outperform-industry-leader-wilson-q331e', 'https://multiplatform.ai/etched-introduces-sohu-chip-pioneering-transformer-exclusive-ai-processing/', 'https://www.linkedin.com/posts/kaliouby_ai-activity-7211393458618458112-AOrB', 'https://www.google.com/search?num=22']\n"
     ]
    }
   ],
   "source": [
    "SA=SummarizationAgent()\n",
    "SA.create_brochure('Zoetis',url, job_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2cc33f-9f87-41ed-b6ec-9d048a9c15f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
